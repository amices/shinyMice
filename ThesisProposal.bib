
@online{181209384Revisiting,
  title = {[1812.09384] {{Revisiting}} the {{Gelman}}-{{Rubin Diagnostic}}},
  url = {https://arxiv.org/abs/1812.09384},
  urldate = {2019-11-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\5LQBDMNE\\1812.html}
}

@online{201539598005Pdf,
  title = {2015-39598-005.Pdf},
  journaltitle = {Google Docs},
  url = {https://drive.google.com/file/d/1igZDoIE-Aly-qXtXyd2YUkDAI3twTVXR/edit?usp=embed_facebook},
  urldate = {2019-12-17},
  file = {C\:\\Users\\User\\Zotero\\storage\\T9L3BUMK\\edit.html}
}

@article{abay08,
  title = {Diagnostics for Multivariate Imputations},
  author = {Abayomi, Kobi and Gelman, Andrew and Levy, Marc},
  date = {2008-06},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  shortjournal = {J Royal Statistical Soc C},
  volume = {57},
  pages = {273--291},
  issn = {0035-9254, 1467-9876},
  doi = {10.1111/j.1467-9876.2007.00613.x},
  abstract = {We consider three sorts of diagnostics for random imputations: displays of the completed data, which are intended to reveal unusual patterns that might suggest problems with the imputations, comparisons of the distributions of observed and imputed data values and checks of the fit of observed data to the model that is used to create the imputations. We formulate these methods in terms of sequential regression multivariate imputation, which is an iterative procedure in which the missing values of each variable are randomly imputed conditionally on all the other variables in the completed data matrix. We also consider a recalibration procedure for sequential regression imputations. We apply these methods to the 2002 environmental sustainability index, which is a linear aggregation of 64 environmental variables on 142 countries.},
  file = {C\:\\Users\\User\\Zotero\\storage\\YHCKE6CH\\Abayomi e.a. - 2008 - Diagnostics for multivariate imputations.pdf},
  langid = {english},
  number = {3}
}

@online{AcfFunctionDocumentation,
  title = {Acf Function | {{R Documentation}}},
  url = {https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/acf},
  urldate = {2020-04-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\GWY8U55V\\acf.html}
}

@book{alli02,
  title = {Missing Data},
  author = {Allison, Paul D.},
  date = {2001},
  publisher = {{Sage publications}},
  file = {C\:\\Users\\User\\Zotero\\storage\\KHTDX984\\books.html}
}

@article{allisonMultipleImputationMissing2000,
  title = {Multiple {{Imputation}} for {{Missing Data}}: {{A Cautionary Tale}}},
  shorttitle = {Multiple {{Imputation}} for {{Missing Data}}},
  author = {ALLISON, PAUL D.},
  date = {2000-02-01},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  volume = {28},
  pages = {301--309},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124100028003003},
  abstract = {Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian bootstrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.},
  file = {C\:\\Users\\User\\Zotero\\storage\\J6TQ4ITP\\ALLISON - 2000 - Multiple Imputation for Missing Data A Cautionary.pdf},
  number = {3}
}

@article{article,
  title = {Comparison of Methodologies to Assess the Convergence of Markov Chain Monte Carlo Methods},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard},
  date = {2006-02},
  journaltitle = {Computational Statistics \& Data Analysis},
  volume = {50},
  pages = {2685--2701},
  doi = {10.1016/j.csda.2005.04.018}
}

@online{ArtificialIntelligenceMachine,
  title = {Artificial Intelligence and Machine Learning in Clinical Development: A Translational Perspective | Npj {{Digital Medicine}}},
  url = {https://www.nature.com/articles/s41746-019-0148-3},
  urldate = {2020-01-22},
  file = {C\:\\Users\\User\\Zotero\\storage\\NT426STC\\s41746-019-0148-3.html}
}

@article{bachrachPersonalityPatternsFacebook,
  title = {Personality and Patterns of {{Facebook}} Usage},
  author = {Bachrach, Yoram and Kosinski, Michal and Graepel, Thore and Kohli, Pushmeet and Stillwell, David},
  pages = {9},
  abstract = {We show how users’ activity on Facebook relates to their personality, as measured by the standard Five Factor Model. Our dataset consists of the personality profiles and Facebook profile data of 180,000 users. We examine correlations between users’ personality and the properties of their Facebook profiles such as the size and density of their friendship network, number uploaded photos, number of events attended, number of group memberships, and number of times user has been tagged in photos. Our results show significant relationships between personality traits and various features of Facebook profiles. We then show how multivariate regression allows prediction of the personality traits of an individual user given their Facebook profile. The best accuracy of such predictions is achieved for Extraversion and Neuroticism, the lowest accuracy is obtained for Agreeableness, with Openness and Conscientiousness lying in the middle.},
  file = {C\:\\Users\\User\\Zotero\\storage\\EUN8NNGG\\Bachrach e.a. - Personality and patterns of Facebook usage.pdf},
  langid = {english}
}

@inproceedings{bachrachPersonalityPatternsFacebook2012,
  title = {Personality and {{Patterns}} of {{Facebook Usage}}},
  booktitle = {Proceedings of the 4th {{Annual ACM Web Science Conference}}},
  author = {Bachrach, Yoram and Kosinski, Michal and Graepel, Thore and Kohli, Pushmeet and Stillwell, David},
  date = {2012},
  pages = {24--32},
  publisher = {{ACM}},
  location = {{New York, NY, USA}},
  doi = {10.1145/2380718.2380722},
  abstract = {We show how users' activity on Facebook relates to their personality, as measured by the standard Five Factor Model. Our dataset consists of the personality profiles and Facebook profile data of 180,000 users. We examine correlations between users' personality and the properties of their Facebook profiles such as the size and density of their friendship network, number uploaded photos, number of events attended, number of group memberships, and number of times user has been tagged in photos. Our results show significant relationships between personality traits and various features of Facebook profiles. We then show how multivariate regression allows prediction of the personality traits of an individual user given their Facebook profile. The best accuracy of such predictions is achieved for Extraversion and Neuroticism, the lowest accuracy is obtained for Agreeableness, with Openness and Conscientiousness lying in the middle.},
  file = {C\:\\Users\\User\\Zotero\\storage\\FI262D7U\\Bachrach e.a. - 2012 - Personality and Patterns of Facebook Usage.pdf},
  isbn = {978-1-4503-1228-8},
  keywords = {big five personality model,Personality,social networks},
  series = {{{WebSci}} '12},
  venue = {Evanston, Illinois}
}

@article{bang05,
  title = {Doubly {{Robust Estimation}} in {{Missing Data}} and {{Causal Inference Models}}},
  author = {Bang, Heejung and Robins, James M.},
  date = {2005},
  journaltitle = {Biometrics},
  volume = {61},
  pages = {962--973},
  issn = {1541-0420},
  doi = {10.1111/j.1541-0420.2005.00377.x},
  abstract = {The goal of this article is to construct doubly robust (DR) estimators in ignorable missing data and causal inference models. In a missing data model, an estimator is DR if it remains consistent when either (but not necessarily both) a model for the missingness mechanism or a model for the distribution of the complete data is correctly specified. Because with observational data one can never be sure that either a missingness model or a complete data model is correct, perhaps the best that can be hoped for is to find a DR estimator. DR estimators, in contrast to standard likelihood-based or (nonaugmented) inverse probability-weighted estimators, give the analyst two chances, instead of only one, to make a valid inference. In a causal inference model, an estimator is DR if it remains consistent when either a model for the treatment assignment mechanism or a model for the distribution of the counterfactual data is correctly specified. Because with observational data one can never be sure that a model for the treatment assignment mechanism or a model for the counterfactual data is correct, inference based on DR estimators should improve upon previous approaches. Indeed, we present the results of simulation studies which demonstrate that the finite sample performance of DR estimators is as impressive as theory would predict. The proposed method is applied to a cardiovascular clinical trial.},
  file = {C\:\\Users\\User\\Zotero\\storage\\KJSVWNQL\\Bang en Robins - 2005 - Doubly Robust Estimation in Missing Data and Causa.pdf;C\:\\Users\\User\\Zotero\\storage\\MVAKGY8K\\j.1541-0420.2005.00377.html},
  keywords = {Causal inference,Doubly robust estimation,Longitudinal data,Marginal structural model,Missing data,Semiparametrics},
  langid = {english},
  number = {4}
}

@article{bart15,
  title = {Multiple Imputation of Covariates by Fully Conditional Specification: {{Accommodating}} the Substantive Model},
  shorttitle = {Multiple Imputation of Covariates by Fully Conditional Specification},
  author = {Bartlett, Jonathan W and Seaman, Shaun R and White, Ian R and Carpenter, James R},
  date = {2015-08-01},
  journaltitle = {Statistical Methods in Medical Research},
  shortjournal = {Stat Methods Med Res},
  volume = {24},
  pages = {462--487},
  issn = {0962-2802},
  doi = {10.1177/0962280214521348},
  abstract = {Missing covariate data commonly occur in epidemiological and clinical research, and are often dealt with using multiple imputation. Imputation of partially observed covariates is complicated if the substantive model is non-linear (e.g. Cox proportional hazards model), or contains non-linear (e.g. squared) or interaction terms, and standard software implementations of multiple imputation may impute covariates from models that are incompatible with such substantive models. We show how imputation by fully conditional specification, a popular approach for performing multiple imputation, can be modified so that covariates are imputed from models which are compatible with the substantive model. We investigate through simulation the performance of this proposal, and compare it with existing approaches. Simulation results suggest our proposal gives consistent estimates for a range of common substantive models, including models which contain non-linear covariate effects or interactions, provided data are missing at random and the assumed imputation models are correctly specified and mutually compatible. Stata software implementing the approach is freely available.},
  file = {C\:\\Users\\User\\Zotero\\storage\\IP6F7U98\\Bartlett e.a. - 2015 - Multiple imputation of covariates by fully conditi.pdf},
  langid = {english},
  number = {4}
}

@article{beaulieu-jonesTrendsFocusMachine2019,
  title = {Trends and {{Focus}} of {{Machine Learning Applications}} for {{Health Research}}},
  author = {Beaulieu-Jones, Brett and Finlayson, Samuel G. and Chivers, Corey and Chen, Irene and McDermott, Matthew and Kandola, Jaz and Dalca, Adrian V. and Beam, Andrew and Fiterau, Madalina and Naumann, Tristan},
  date = {2019-10-02},
  journaltitle = {JAMA Network Open},
  shortjournal = {JAMA Netw Open},
  volume = {2},
  pages = {e1914051-e1914051},
  doi = {10.1001/jamanetworkopen.2019.14051},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}The use of machine learning applications related to health is rapidly increasing and may have the potential to profoundly affect the field of health care.{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To analyze submissions to a popular machine learning for health venue to assess the current state of research, including areas of methodologic and clinical focus, limitations, and underexplored areas.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$><$}p{$>$}In this data-driven qualitative analysis, 166 accepted manuscript submissions to the Third Annual Machine Learning for Health workshop at the 32nd Conference on Neural Information Processing Systems on December 8, 2018, were analyzed to understand research focus, progress, and trends. Experts reviewed each submission against a rubric to identify key data points, statistical modeling and analysis of submitting authors was performed, and research topics were quantitatively modeled. Finally, an iterative discussion of topics common in submissions and invited speakers at the workshop was held to identify key trends.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}Frequency and statistical measures of methods, topics, goals, and author attributes were derived from an expert review of submissions guided by a rubric.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}Of the 166 accepted submissions, 58 (34.9\%) had clinician involvement and 83 submissions (50.0\%) that focused on clinical practice included clinical collaborators. A total of 97 data sets (58.4\%) used in submissions were publicly available or required a standard registration process. Clinical practice was the most common application area (70 manuscripts [42.2\%]), with brain and mental health (25 [15.1\%]), oncology (21 [12.7\%]), and cardiovascular (19 [11.4\%]) being the most common specialties.{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}Trends in machine learning for health research indicate the importance of well-annotated, easily accessed data and the benefit from greater clinician involvement in the development of translational applications.{$<$}/p{$>$}},
  file = {C\:\\Users\\User\\Zotero\\storage\\GCD6ASKQ\\Beaulieu-Jones e.a. - 2019 - Trends and Focus of Machine Learning Applications .pdf;C\:\\Users\\User\\Zotero\\storage\\LFCD4KVG\\2753523.html},
  langid = {english},
  number = {10}
}

@article{benthallPhilosophyComputationalSocial2016,
  title = {Philosophy of {{Computational Social Science}}},
  author = {Benthall, Sebastian},
  date = {2016-10-11},
  journaltitle = {Cosmos and History: The Journal of Natural and Social Philosophy},
  volume = {12},
  pages = {13-30-30},
  issn = {1832-9101},
  url = {http://cosmosandhistory.org/index.php/journal/article/view/570},
  urldate = {2020-01-19},
  abstract = {Computational social science is an emerging field at the intersection of statistics, computer science, and the social sciences. This paper addresses the philosophical foundations of this new field. Kant and Peirce provide an understanding of scientific objectivity as intersubjective validity. Modern mathematics, and especially the mathematics of algorithms and statistics, get their objectivity from the intersubjective validity of formal proof. Algorithms implementing statistical inference, or  scientific algorithms , are what distinguish computational social science epistemically from other social sciences. This gives computational social science an objective validity that other social sciences do not have. Objections to the scientific realism of this philosophy from the positions of anti-instrumentalism, postmodern interpretivism, and situated epistemology are considered and either incorporated into this philosophy of computational social science or refuted. Speculative predictions for the field of computational social science are offered in conclusion: computational social science will bring about an end of narrative in the social sciences, contract the field of social scientific knowledge into a narrower, more hierarchical field of expertise, and create a democratic crisis that will only be resolved through universal education in computational statistics.       Normal   0               false   false   false      EN-US   X-NONE   X-NONE             /* Style Definitions */  table.MsoNormalTable 	\{mso-style-name:"Table Normal"; 	mso-tstyle-rowband-size:0; 	mso-tstyle-colband-size:0; 	mso-style-noshow:yes; 	mso-style-priority:99; 	mso-style-parent:""; 	mso-padding-alt:0cm 5.4pt 0cm 5.4pt; 	mso-para-margin:0cm; 	mso-para-margin-bottom:.0001pt; 	mso-pagination:widow-orphan; 	font-size:10.0pt; 	font-family:"Times New Roman","serif"; 	mso-ansi-language:EN-US; 	mso-fareast-language:EN-US;\}},
  file = {C\:\\Users\\User\\Zotero\\storage\\273K2EVM\\Benthall - 2016 - Philosophy of Computational Social Science.pdf;C\:\\Users\\User\\Zotero\\storage\\D6N3JDHR\\570.html},
  langid = {english},
  number = {2}
}

@book{berryBayesianAnalysisStatistics1996,
  title = {Bayesian {{Analysis}} in {{Statistics}} and {{Econometrics}}: {{Essays}} in {{Honor}} of {{Arnold Zellner}}},
  shorttitle = {Bayesian {{Analysis}} in {{Statistics}} and {{Econometrics}}},
  author = {Berry, Donald A. and Chaloner, Kathryn M. and Geweke, John K. and Zellner, Arnold},
  date = {1996},
  publisher = {{John Wiley \& Sons}},
  abstract = {A definitive work which captures the current state of knowledge in these disciplines and sheds new light on differences and similarities between the Bayesian and Frequentist approaches. Contains a collection of original papers dealing with such topics as inferential matters, the role of prior distributions, regression and related problems, model selection, computational issues, diverse types of applications, and much more.},
  eprint = {oyiYbc_h8S4C},
  eprinttype = {googlebooks},
  isbn = {978-0-471-11856-5},
  keywords = {Business & Economics / Econometrics,Mathematics / Probability & Statistics / Stochastic Processes},
  langid = {english},
  pagetotal = {610}
}

@article{bertsimasPredictiveMethodsMissing2018,
  title = {From {{Predictive Methods}} to {{Missing Data Imputation}}: {{An Optimization Approach}}},
  shorttitle = {From {{Predictive Methods}} to {{Missing Data Imputation}}},
  author = {Bertsimas, Dimitris and Pawlowski, Colin and Zhuo, Ying Daisy},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--39},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-073.html},
  urldate = {2020-01-24},
  file = {C\:\\Users\\User\\Zotero\\storage\\KW3EWGUZ\\Bertsimas e.a. - 2018 - From Predictive Methods to Missing Data Imputation.pdf;C\:\\Users\\User\\Zotero\\storage\\UND8URTJ\\17-073.html},
  number = {196}
}

@article{bertsimasPredictiveMethodsMissing2018a,
  title = {From {{Predictive Methods}} to {{Missing Data Imputation}}: {{An Optimization Approach}}},
  shorttitle = {From {{Predictive Methods}} to {{Missing Data Imputation}}},
  author = {Bertsimas, Dimitris and Pawlowski, Colin and Zhuo, Ying Daisy},
  date = {2018},
  journaltitle = {Journal of Machine Learning Research},
  volume = {18},
  pages = {1--39},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v18/17-073.html},
  urldate = {2020-01-24},
  file = {C\:\\Users\\User\\Zotero\\storage\\EMB4XFV8\\Bertsimas e.a. - 2018 - From Predictive Methods to Missing Data Imputation.pdf;C\:\\Users\\User\\Zotero\\storage\\KF875VL7\\17-073.html},
  number = {196}
}

@book{billigLearnWriteBadly2013,
  title = {Learn to Write Badly: How to Succeed in the Social Sciences},
  shorttitle = {Learn to Write Badly},
  author = {Billig, Michael},
  date = {2013-01-01},
  publisher = {{Loughborough University}},
  url = {https://repository.lboro.ac.uk/articles/Learn_to_write_badly_how_to_succeed_in_the_social_sciences/9471617},
  urldate = {2020-01-20},
  abstract = {Modern academia is increasingly competitive yet the writing style of social scientists is routinely poor and continues to deteriorate. Are social science postgraduates being taught to write poorly? What conditions adversely affect the way they write? And which linguistic features contribute towards this bad writing? Michael Billig's witty and entertaining book analyses these questions in a quest to pinpoint exactly what is going wrong with the way social scientists write. Using examples from diverse fields such as linguistics, sociology and experimental social psychology, Billig shows how technical terminology is regularly less precise than simpler language. He demonstrates that there are linguistic problems with the noun-based terminology that social scientists habitually use - 'reification' or 'nominalization' rather than the corresponding verbs 'reify' or 'nominalize'. According to Billig, social scientists not only use their terminology to exaggerate and to conceal, but also to promote themselves and their work.},
  file = {C\:\\Users\\User\\Zotero\\storage\\8ADPWGKK\\Billig - 2013 - Learn to write badly how to succeed in the social.html},
  isbn = {978-1-107-67698-5},
  langid = {english}
}

@article{billigLearnWriteBadly2013a,
  title = {Learn to Write Badly: {{How}} to Succeed in the Social Sciences},
  author = {Billig, Michael},
  date = {2013},
  journaltitle = {Cambridge, UK: Cambridge},
  shortjournal = {Cambridge, UK: Cambridge}
}

@book{blackburnThinkCompellingIntroduction1999,
  title = {Think: {{A}} Compelling Introduction to Philosophy},
  author = {Blackburn, Simon},
  date = {1999},
  publisher = {{Oxford University Press}},
  isbn = {0-19-976984-2}
}

@article{bond16,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models},
  author = {Bondarenko, Irina and Raghunathan, Trivellore},
  date = {2016},
  journaltitle = {Statistics in Medicine},
  volume = {35},
  pages = {3007--3020},
  issn = {1097-0258},
  doi = {10.1002/sim.6926},
  abstract = {Multiple imputation has become a popular approach for analyzing incomplete data. Many software packages are available to multiply impute the missing values and to analyze the resulting completed data sets. However, diagnostic tools to check the validity of the imputations are limited, and the majority of the currently available methods need considerable knowledge of the imputation model. In many practical settings, however, the imputer and the analyst may be different individuals or from different organizations, and the analyst model may or may not be congenial to the model used by the imputer. This article develops and evaluates a set of graphical and numerical diagnostic tools for two practical purposes: (i) for an analyst to determine whether the imputations are reasonable under his/her model assumptions without actually knowing the imputation model assumptions; and (ii) for an imputer to fine tune the imputation model by checking the key characteristics of the observed and imputed values. The tools are based on the numerical and graphical comparisons of the distributions of the observed and imputed values conditional on the propensity of response. The methodology is illustrated using simulated data sets created under a variety of scenarios. The examples focus on continuous and binary variables, but the principles can be used to extend methods for other types of variables. Copyright © 2016 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\User\\Zotero\\storage\\BBYIKCGA\\Bondarenko en Raghunathan - 2016 - Graphical and numerical diagnostic tools to assess.pdf;C\:\\Users\\User\\Zotero\\storage\\Q53PV2BZ\\sim.html},
  keywords = {congeniality,diagnostics,multiple imputation,propensity score},
  langid = {english},
  number = {17}
}

@book{book,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, Daniel and Martin, James},
  date = {2019-02},
  volume = {2}
}

@online{BorensteinHedgesHiggins,
  title = {Borenstein, Hedges, Higgins, \& Rothstein - Introduction to Meta-Analysis.Pdf},
  journaltitle = {Google Docs},
  url = {https://drive.google.com/a/students.uu.nl/file/d/1ok_vPvjRXlXlFkc0vNCdf0Su64kfHfMd/view?usp=drive_open&usp=embed_facebook},
  urldate = {2019-06-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZFREFRTJ\\borenstein, hedges, higgins, & rothstein - introdu.html}
}

@book{borensteinIntroductionMetaanalysis2009,
  title = {Introduction to Meta-Analysis},
  editor = {Borenstein, Michael},
  date = {2009},
  publisher = {{John Wiley \& Sons}},
  location = {{Chichester, U.K}},
  abstract = {This text provides a concise and clearly presented discussion of all the elements in a meta-analysis. It is illustrated with worked examples throughout, with visual explanations, using screenshots from Excel spreadsheets and computer programs such as Comprehensive Meta-Analysis (CMA) or Strata},
  annotation = {OCLC: ocn263294996},
  file = {C\:\\Users\\User\\Zotero\\storage\\WLFD7PSX\\Borenstein - 2009 - Introduction to meta-analysis.pdf},
  isbn = {978-0-470-05724-7},
  keywords = {Meta-analysis,Meta-Analysis as Topic},
  langid = {english},
  pagetotal = {421}
}

@book{box15,
  title = {Time {{Series Analysis}}: {{Forecasting}} and {{Control}}},
  shorttitle = {Time {{Series Analysis}}},
  author = {Box, George E. P. and Jenkins, Gwilym M. and Reinsel, Gregory C. and Ljung, Greta M.},
  date = {2015-05-29},
  publisher = {{John Wiley \& Sons}},
  abstract = {Praise for the Fourth Edition  "The book follows faithfully the style of the original edition. The approach is heavily motivated by real-world time series, and by developing a complete approach to model building, estimation, forecasting and control."—Mathematical Reviews Bridging classical models and modern topics, the Fifth Edition of Time Series Analysis: Forecasting and Control maintains a balanced presentation of the tools for modeling and analyzing time series. Also describing the latest developments that have occurred in the field over the past decade through applications from areas such as business, finance, and engineering, the Fifth Edition continues to serve as one of the most influential and prominent works on the subject. Time Series Analysis: Forecasting and Control, Fifth Edition provides a clearly written exploration of the key methods for building, classifying, testing, and analyzing stochastic models for time series and describes their use in five important areas of application: forecasting; determining the transfer function of a system; modeling the effects of intervention events; developing multivariate dynamic models; and designing simple control schemes. Along with these classical uses, the new edition covers modern topics with new features that include:  A redesigned chapter on multivariate time series analysis with an expanded treatment of Vector Autoregressive, or VAR models, along with a discussion of the analytical tools needed for modeling vector time series An expanded chapter on special topics covering unit root testing, time-varying volatility models such as ARCH and GARCH, nonlinear time series models, and long memory models Numerous examples drawn from finance, economics, engineering, and other related fields The use of the publicly available R software for graphical illustrations and numerical calculations along with scripts that demonstrate the use of R for model building and forecasting Updates to literature references throughout and new end-of-chapter exercises Streamlined chapter introductions and revisions that update and enhance the exposition  Time Series Analysis: Forecasting and Control, Fifth Edition is a valuable real-world reference for researchers and practitioners in time series analysis, econometrics, finance, and related fields. The book is also an excellent textbook for beginning graduate-level courses in advanced statistics, mathematics, economics, finance, engineering, and physics.},
  eprint = {rNt5CgAAQBAJ},
  eprinttype = {googlebooks},
  isbn = {978-1-118-67492-5},
  keywords = {Mathematics / Probability & Statistics / General,Mathematics / Probability & Statistics / Stochastic Processes},
  langid = {english},
  pagetotal = {709}
}

@article{brandToolkitSASEvaluation2003,
  title = {A Toolkit in {{SAS}} for the Evaluation of Multiple Imputation Methods},
  author = {Brand, Jaap P. L. and van Buuren, Stef and Groothuis‐Oudshoorn, Karin and Gelsema, Edzard S.},
  date = {2003},
  journaltitle = {Statistica Neerlandica},
  volume = {57},
  pages = {36--45},
  issn = {1467-9574},
  doi = {10.1111/1467-9574.00219},
  abstract = {This paper outlines a strategy to validate multiple imputation methods. Rubin's criteria for proper multiple imputation are the point of departure. We describe a simulation method that yields insight into various aspects of bias and efficiency of the imputation process. We propose a new method for creating incomplete data under a general Missing At Random (MAR) mechanism. Software implementing the validation strategy is available as a SAS/IML module. The method is applied to investigate the behavior of polytomous regression imputation for categorical data.},
  file = {C\:\\Users\\User\\Zotero\\storage\\VSXG993W\\1467-9574.html},
  keywords = {missing data mechanism,multiple imputation,proper imputation,simulation.},
  langid = {english},
  number = {1}
}

@article{brig03,
  title = {Missing.... Presumed at Random: Cost-Analysis of Incomplete Data},
  shorttitle = {Missing.... Presumed at Random},
  author = {Briggs, Andrew and Clark, Taane and Wolstenholme, Jane and Clarke, Philip},
  date = {2003},
  journaltitle = {Health Economics},
  volume = {12},
  pages = {377--392},
  issn = {1099-1050},
  doi = {10.1002/hec.766},
  abstract = {When collecting patient-level resource use data for statistical analysis, for some patients and in some categories of resource use, the required count will not be observed. Although this problem must arise in most reported economic evaluations containing patient-level data, it is rare for authors to detail how the problem was overcome. Statistical packages may default to handling missing data through a so-called ‘complete case analysis’, while some recent cost-analyses have appeared to favour an ‘available case’ approach. Both of these methods are problematic: complete case analysis is inefficient and is likely to be biased; available case analysis, by employing different numbers of observations for each resource use item, generates severe problems for standard statistical inference. Instead we explore imputation methods for generating ‘replacement’ values for missing data that will permit complete case analysis using the whole data set and we illustrate these methods using two data sets that had incomplete resource use information. Copyright © 2002 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\User\\Zotero\\storage\\69EUE4JH\\Briggs e.a. - 2003 - Missing.... presumed at random cost-analysis of i.pdf;C\:\\Users\\User\\Zotero\\storage\\LBI3Q8QB\\hec.html},
  keywords = {cost-analysis,economic evaluation,missing data},
  langid = {english},
  number = {5}
}

@article{broo00,
  title = {Markov {{Chain Monte Carlo Convergence Assessment}} via {{Two}}-{{Way Analysis}} of {{Variance}}},
  author = {Brooks, S. P. and Giudici, P.},
  date = {2000},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {9},
  pages = {266--285},
  issn = {1061-8600},
  doi = {10.2307/1390654},
  abstract = {In this article we discuss the problem of assessing the performance of Markov chain Monte Carlo (MCMC) algorithms on the basis of simulation output. In essence, we extend the original ideas of Gelman and Rubin and, more recently, Brooks and Gelman, to problems where we are able to split the variation inherent within the MCMC simulation output into two distinct groups. We show how such a diagnostic may be useful in assessing the performance of MCMC samplers addressing model choice problems, such as the reversible jump MCMC algorithm. In the model choice context, we show how the reversible jump MCMC simulation output for parameters that retain a coherent interpretation throughout the simulation, can be used to assess convergence. By considering various decompositions of the sampling variance of this parameter, we can assess the performance of our MCMC sampler in terms of its mixing properties both within and between models and we illustrate our approach in both the graphical Gaussian models and normal mixtures context. Finally, we provide an example of the application of our diagnostic to the assessment of the influence of different starting values on MCMC simulation output, thereby illustrating the wider utility of our method beyond the Bayesian model choice and reversible jump MCMC context.},
  eprint = {1390654},
  eprinttype = {jstor},
  number = {2}
}

@article{broo98,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {7},
  pages = {434--455},
  doi = {10.1080/10618600.1998.10474787},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {C\:\\Users\\User\\Zotero\\storage\\Q7I49K25\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\User\\Zotero\\storage\\SDA8QD6T\\brooksgelman2.pdf;C\:\\Users\\User\\Zotero\\storage\\5Q2ITJG5\\10618600.1998.html},
  keywords = {Convergence diagnosis,Inference,Markov chain Monte Carlo},
  number = {4}
}

@article{brooksGeneralMethodsMonitoring1998,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  volume = {7},
  pages = {434--455},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {C\:\\Users\\User\\Zotero\\storage\\N4XJMNKZ\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\User\\Zotero\\storage\\YK6T84HC\\10618600.1998.html},
  keywords = {Convergence diagnosis,Inference,Markov chain Monte Carlo},
  number = {4}
}

@article{brooksGeneralMethodsMonitoring1998a,
  title = {General {{Methods}} for {{Monitoring Convergence}} of {{Iterative Simulations}}},
  author = {Brooks, Stephen P. and Gelman, Andrew},
  date = {1998-12-01},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {7},
  pages = {434--455},
  publisher = {{Taylor \& Francis}},
  issn = {1061-8600},
  doi = {10.1080/10618600.1998.10474787},
  abstract = {We generalize the method proposed by Gelman and Rubin (1992a) for monitoring the convergence of iterative simulations by comparing between and within variances of multiple chains, in order to obtain a family of tests for convergence. We review methods of inference from simulations in order to develop convergence-monitoring summaries that are relevant for the purposes for which the simulations are used. We recommend applying a battery of tests for mixing based on the comparison of inferences from individual sequences and from the mixture of sequences. Finally, we discuss multivariate analogues, for assessing convergence of several parameters simultaneously.},
  file = {C\:\\Users\\User\\Zotero\\storage\\HVRQC6HJ\\Brooks en Gelman - 1998 - General Methods for Monitoring Convergence of Iter.pdf;C\:\\Users\\User\\Zotero\\storage\\HCJKPR36\\10618600.1998.html},
  number = {4}
}

@article{buckleyChinaPushTurn2019,
  title = {Inside {{China}}’s {{Push}} to {{Turn Muslim Minorities Into}} an {{Army}} of {{Workers}}},
  author = {Buckley, Chris and Ramzy, Austin},
  date = {2019-12-30T09:31:03-05:00},
  journaltitle = {The New York Times},
  issn = {0362-4331},
  url = {https://www.nytimes.com/2019/12/30/world/asia/china-xinjiang-muslims-labor.html},
  urldate = {2020-04-13},
  abstract = {The Communist Party wants to remold Xinjiang’s minorities into loyal blue-collar workers to supply Chinese factories with cheap labor.},
  entrysubtype = {newspaper},
  journalsubtitle = {World},
  keywords = {China,Communist Party of China,Factories and Manufacturing,Forced Labor,Human Rights and Human Rights Violations,International Trade and World Market,Kazakhs (Ethnic Group),Politics and Government,Qapqal County (China),Uighurs (Chinese Ethnic Group),Xinjiang (China)},
  langid = {american}
}

@book{buur18,
  title = {Flexible Imputation of Missing Data},
  author = {Van Buuren, Stef},
  date = {2018},
  publisher = {{Chapman and Hall/CRC}}
}

@article{buurenFullyConditionalSpecification2006,
  title = {Fully Conditional Specification in Multivariate Imputation},
  author = {Buuren, S. Van and Brand, J. P. L. and Groothuis-Oudshoorn, C. G. M. and Rubin, D. B.},
  date = {2006-12-01},
  journaltitle = {Journal of Statistical Computation and Simulation},
  volume = {76},
  pages = {1049--1064},
  publisher = {{Taylor \& Francis}},
  issn = {0094-9655},
  doi = {10.1080/10629360600810434},
  abstract = {The use of the Gibbs sampler with fully conditionally specified models, where the distribution of each variable given the other variables is the starting point, has become a popular method to create imputations in incomplete multivariate data. The theoretical weakness of this approach is that the specified conditional densities can be incompatible, and therefore the stationary distribution to which the Gibbs sampler attempts to converge may not exist. This study investigates practical consequences of this problem by means of simulation. Missing data are created under four different missing data mechanisms. Attention is given to the statistical behavior under compatible and incompatible models. The results indicate that multiple imputation produces essentially unbiased estimates with appropriate coverage in the simple cases investigated, even for the incompatible models. Of particular interest is that these results were produced using only five Gibbs iterations starting from a simple draw from observed marginal distributions. It thus appears that, despite the theoretical weaknesses, the actual performance of conditional model specification for multivariate imputation can be quite good, and therefore deserves further study.},
  annotation = {\_eprint: https://doi.org/10.1080/10629360600810434},
  file = {C\:\\Users\\User\\Zotero\\storage\\YQ2Q2H3J\\Buuren e.a. - 2006 - Fully conditional specification in multivariate im.pdf;C\:\\Users\\User\\Zotero\\storage\\5VUVSNA2\\10629360600810434.html},
  keywords = {Distributional compatibility,Gibbs sampling,Multiple imputation,Multivariate missing data,Proper imputation,Simulation},
  number = {12}
}

@article{buurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {Buuren, S Van and Boshuizen, H C and Knook, D L},
  date = {1999},
  pages = {14},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85\# years), blood pressure measurements are missing in about 12)5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records. Copyright 1999 John Wiley \& Sons, Ltd.},
  file = {C\:\\Users\\User\\Zotero\\storage\\WSSMFDNU\\Multiple imputation - Stat Med 1999.pdf},
  langid = {english}
}

@book{Cameron+Trivedi:2013,
  title = {Regression Analysis of Count Data},
  author = {Cameron, A. Colin and Trivedi, Pravin K.},
  date = {2013},
  edition = {2},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}}
}

@article{campbellCanWeDisregard2019,
  title = {Can We Disregard the Whole Model? {{Omnibus}} Non-Inferiority Testing for \${{R}}\^\{2\}\$ in Multivariable Linear Regression and \$\textbackslash hat\{\textbackslash eta\}\^\{2\}\$ in {{ANOVA}}},
  shorttitle = {Can We Disregard the Whole Model?},
  author = {Campbell, Harlan and Lakens, Daniël},
  date = {2019-05-28},
  url = {http://arxiv.org/abs/1905.11875},
  urldate = {2019-06-13},
  abstract = {Determining a lack of association between an outcome variable and a number of different explanatory variables is frequently necessary in order to disregard a proposed model (i.e., to confirm the lack of an association between an outcome and predictors). Despite this, the literature rarely offers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. This paper introduces non-inferiority tests for ANOVA and linear regression analyses, that correspond to the standard widely used \$F\$-test for \$\textbackslash hat\{\textbackslash eta\}\^2\$ and \$R\^\{2\}\$, respectively. A simulation study is conducted to examine the type I error rates and statistical power of the tests, and a comparison is made with an alternative Bayesian testing approach. The results indicate that the proposed non-inferiority test is a potentially useful tool for 'testing the null.'},
  archivePrefix = {arXiv},
  eprint = {1905.11875},
  eprinttype = {arxiv},
  keywords = {Statistics - Applications,Statistics - Methodology},
  primaryClass = {stat}
}

@article{campbellCanWeDisregard2019a,
  title = {Can We Disregard the Whole Model? {{Omnibus}} Non-Inferiority Testing for \${{R}}\^\{2\}\$ in Multivariable Linear Regression and \$\textbackslash hat\{\textbackslash eta\}\^\{2\}\$ in {{ANOVA}}},
  shorttitle = {Can We Disregard the Whole Model?},
  author = {Campbell, Harlan and Lakens, Daniël},
  date = {2019-05-28},
  url = {http://arxiv.org/abs/1905.11875},
  urldate = {2019-06-19},
  abstract = {Determining a lack of association between an outcome variable and a number of different explanatory variables is frequently necessary in order to disregard a proposed model (i.e., to confirm the lack of an association between an outcome and predictors). Despite this, the literature rarely offers information about, or technical recommendations concerning, the appropriate statistical methodology to be used to accomplish this task. This paper introduces non-inferiority tests for ANOVA and linear regression analyses, that correspond to the standard widely used \$F\$-test for \$\textbackslash hat\{\textbackslash eta\}\^2\$ and \$R\^\{2\}\$, respectively. A simulation study is conducted to examine the type I error rates and statistical power of the tests, and a comparison is made with an alternative Bayesian testing approach. The results indicate that the proposed non-inferiority test is a potentially useful tool for 'testing the null.'},
  archivePrefix = {arXiv},
  eprint = {1905.11875},
  eprinttype = {arxiv},
  file = {C\:\\Users\\User\\Zotero\\storage\\VE9KSLG4\\Campbell en Lakens - 2019 - Can we disregard the whole model Omnibus non-infe.pdf;C\:\\Users\\User\\Zotero\\storage\\65D7IHLD\\1905.html},
  keywords = {Statistics - Applications,Statistics - Methodology},
  primaryClass = {stat}
}

@book{Chambers+Hastie:1992,
  title = {Statistical Models In},
  editor = {Chambers, John M. and Hastie, Trevor J.},
  date = {1992},
  publisher = {{Chapman \& Hall}},
  location = {{London}}
}

@article{changShinyWebApplication2018,
  title = {Shiny: {{Web}} Application Framework for r, 2015},
  author = {Chang, Winston and Cheng, Joe and Allaire, J and Xie, Yihui and McPherson, Jonathan},
  date = {2018},
  journaltitle = {R package version},
  shortjournal = {R package version},
  volume = {1},
  pages = {14},
  number = {0}
}

@incollection{cherkasskyPredictiveLearningKnowledge2012,
  title = {Predictive {{Learning}}, {{Knowledge Discovery}} and {{Philosophy}} of {{Science}}},
  booktitle = {Advances in {{Computational Intelligence}}: {{IEEE World Congress}} on {{Computational Intelligence}}, {{WCCI}} 2012, {{Brisbane}}, {{Australia}}, {{June}} 10-15, 2012. {{Plenary}}/{{Invited Lectures}}},
  author = {Cherkassky, Vladimir},
  editor = {Liu, Jing and Alippi, Cesare and Bouchon-Meunier, Bernadette and Greenwood, Garrison W. and Abbass, Hussein A.},
  date = {2012},
  pages = {209--233},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-30687-7_11},
  abstract = {Various disciplines, such as machine learning, statistics, data mining and artificial neural networks, are concerned with estimation of data-analytic models. A common theme among all these methodologies is estimation of predictive models from data. In our digital age, an abundance of data and cheap computing power offers hope of knowledge discovery via application of statistical and machine learning algorithms to empirical data. This data-analytic knowledge has similarities and differences with classical scientific knowledge. For example, any scientific theory can be viewed as an inductive theory because it generalizes over a finite number of observations (or experiments). The philosophical aspects of induction and knowledge discovery have been thoroughly explored in Western philosophy of science. This philosophical analysis dates back to Kant and Hume. Any knowledge involves a combination of hypotheses/ideas and empirical data. In the modern digital age, the balance between ideas (mental constructs) and observed data (facts) has completely shifted. Classical scientific knowledge was produced mainly by a stroke of genius (e.g., Newton, Maxwell, and Einstein). In contrast, much of modern knowledge in life sciences and social sciences is derived via data-analytic modeling. We argue that such data-driven knowledge can be properly described following the methodology of predictive learning originally developed in VC-theory. This paper presents a brief survey of the philosophical concepts related to inductive inference, and then extends these ideas to predictive data-analytic knowledge discovery. We contrast the differences between classical first-principle knowledge, data-analytic knowledge and beliefs. Several application examples are used to illustrate the differences between classical statistical and predictive learning approaches to data-analytic modeling. Finally, we discuss interpretation of data-analytic models under predictive learning framework.},
  file = {C\:\\Users\\User\\Zotero\\storage\\B5GZFMY9\\Cherkassky - 2012 - Predictive Learning, Knowledge Discovery and Philo.pdf},
  isbn = {978-3-642-30687-7},
  keywords = {Empirical Knowledge,Knowledge Discovery,Mutual Fund,Support Vector Machine,Support Vector Machine Model},
  langid = {english},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@book{china.FundamentalLegalDocuments1962,
  title = {Fundamental Legal Documents of {{Communist China}}},
  author = {{China.} and Blaustein, Albert P.},
  date = {1962},
  publisher = {{F. B. Rothman}},
  location = {{South Hackensack, N.J.}},
  url = {https://catalog.hathitrust.org/Record/001625431},
  urldate = {2020-04-13},
  file = {C\:\\Users\\User\\Zotero\\storage\\RMERHDNP\\001625431.html},
  pagetotal = {xxix, 603 p.}
}

@online{ChinaCampaignRepression,
  title = {China’s {{Campaign}} of {{Repression Against Xinjiang}}’s {{Muslims}} | {{HRW}}},
  url = {https://www.hrw.org/report/2018/09/09/eradicating-ideological-viruses/chinas-campaign-repression-against-xinjiangs},
  urldate = {2020-04-13},
  file = {C\:\\Users\\User\\Zotero\\storage\\QUD4QX5Q\\chinas-campaign-repression-against-xinjiangs.html}
}

@book{cohenStatisticalPowerAnalysis1988,
  title = {Statistical Power Analysis for the Behavioral Sciences},
  author = {Cohen, Jacob},
  date = {1988},
  edition = {2nd ed},
  publisher = {{L. Erlbaum Associates}},
  location = {{Hillsdale, N.J}},
  file = {C\:\\Users\\User\\Zotero\\storage\\5YXGF8P9\\Cohen - 1988 - Statistical power analysis for the behavioral scie.pdf},
  isbn = {978-0-8058-0283-2},
  keywords = {Probabilities,Social sciences,Statistical methods,Statistical power analysis},
  langid = {english},
  pagetotal = {567}
}

@online{ComparisonMethodologiesAssess,
  title = {(1) {{Comparison}} of {{Methodologies}} to {{Assess}} the {{Convergence}} of {{Markov Chain Monte Carlo Methods}} | {{Request PDF}}},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/223879476_Comparison_of_Methodologies_to_Assess_the_Convergence_of_Markov_Chain_Monte_Carlo_Methods},
  urldate = {2019-10-15},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\User\\Zotero\\storage\\YMFB3DVA\\(1) Comparison of Methodologies to Assess the Conv.pdf;C\:\\Users\\User\\Zotero\\storage\\8RT5ICLI\\223879476_Comparison_of_Methodologies_to_Assess_the_Convergence_of_Markov_Chain_Monte_Carlo_Met.html},
  langid = {english}
}

@online{ContentsArchivesDirect,
  title = {Contents - {{Archives Direct}} - {{Adam Matthew Digital}}},
  url = {http://www.archivesdirect.amdigital.co.uk.proxy.library.uu.nl/Documents?filter=collection_eq_Foreign+Office+Files+for+China%2c+1919-1980},
  urldate = {2020-03-20},
  file = {C\:\\Users\\User\\Zotero\\storage\\E4C35HNN\\Documents.html}
}

@article{cowl96,
  title = {Markov Chain {{Monte Carlo}} Convergence Diagnostics: A Comparative Review},
  author = {Cowles, Mary Kathryn and Carlin, Bradley P},
  date = {1996},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {91},
  pages = {883--904},
  file = {C\:\\Users\\User\\Zotero\\storage\\JIBHQIG4\\CowlesCarlin.1996.pdf},
  number = {434}
}

@article{crownPotentialApplicationMachine2015,
  title = {Potential {{Application}} of {{Machine Learning}} in {{Health Outcomes Research}} and {{Some Statistical Cautions}}},
  author = {Crown, William H.},
  date = {2015-03-01},
  journaltitle = {Value in Health},
  shortjournal = {Value in Health},
  volume = {18},
  pages = {137--140},
  issn = {1098-3015},
  doi = {10.1016/j.jval.2014.12.005},
  abstract = {Traditional analytic methods are often ill-suited to the evolving world of health care big data characterized by massive volume, complexity, and velocity. In particular, methods are needed that can estimate models efficiently using very large datasets containing healthcare utilization data, clinical data, data from personal devices, and many other sources. Although very large, such datasets can also be quite sparse (e.g., device data may only be available for a small subset of individuals), which creates problems for traditional regression models. Many machine learning methods address such limitations effectively but are still subject to the usual sources of bias that commonly arise in observational studies. Researchers using machine learning methods such as lasso or ridge regression should assess these models using conventional specification tests.},
  file = {C\:\\Users\\User\\Zotero\\storage\\EHC6LY4Z\\Crown - 2015 - Potential Application of Machine Learning in Healt.pdf;C\:\\Users\\User\\Zotero\\storage\\GHNG3BIS\\S1098301514047913.html},
  keywords = {machine learning,outcomes research,treatment effects},
  langid = {english},
  number = {2}
}

@article{cummingPrimerUnderstandingUse,
  title = {A {{Primer}} on the {{Understanding}}, {{Use}}, and {{Calculation}} of {{Confidence Intervals}} That Are {{Based}} on {{Central}} and {{Noncentral Distributions}}},
  author = {Cumming, Geoff and Finch, Sue},
  journaltitle = {EDUCATIONAL AND PSYCHOLOGICAL MEASUREMENT},
  pages = {43},
  file = {C\:\\Users\\User\\Zotero\\storage\\A9YDW324\\Cumming en Finch - A Primer on the Understanding, Use, and Calculatio.pdf},
  langid = {english}
}

@book{cummingUnderstandingNewStatistics2013,
  title = {Understanding {{The New Statistics}}: {{Effect Sizes}}, {{Confidence Intervals}}, and {{Meta}}-{{Analysis}}},
  shorttitle = {Understanding {{The New Statistics}}},
  author = {Cumming, Geoff},
  date = {2013-06-19},
  edition = {1},
  publisher = {{Routledge}},
  doi = {10.4324/9780203807002},
  file = {C\:\\Users\\User\\Zotero\\storage\\8XCE5YCB\\Cumming - 2013 - Understanding The New Statistics Effect Sizes, Co.pdf},
  isbn = {978-0-203-80700-2},
  langid = {english}
}

@article{curranNoncentralChisquareDistribution2002,
  title = {The {{Noncentral Chi}}-Square {{Distribution}} in {{Misspecified Structural Equation Models}}: {{Finite Sample Results}} from a {{Monte Carlo Simulation}}},
  shorttitle = {The {{Noncentral Chi}}-Square {{Distribution}} in {{Misspecified Structural Equation Models}}},
  author = {Curran, Patrick J. and Bollen, Kenneth A. and Paxton, Pamela and Kirby, James and Chen, Feinian},
  date = {2002-01-01},
  journaltitle = {Multivariate Behavioral Research},
  volume = {37},
  pages = {1--36},
  issn = {0027-3171},
  doi = {10.1207/S15327906MBR3701_01},
  abstract = {The noncentral chi-square distribution plays a key role in structural equation modeling (SEM). The likelihood ratio test statistic that accompanies virtually all SEMs asymptotically follows a noncentral chi-square under certain assumptions relating to misspecification and multivariate distribution. Many scholars use the noncentral chi-square distribution in the construction of fit indices, such as Steiger and Lind's (1980) Root Mean Square Error of Approximation (RMSEA) or the family of baseline fit indices (e.g., RNI, CFI), and for the computation of statistical power for model hypothesis testing. Despite this wide use, surprisingly little is known about the extent to which the test statistic follows a noncentral chi-square in applied research. Our study examines several hypotheses about the suitability of the noncentral chi-square distribution for the usual SEM test statistic under conditions commonly encountered in practice. We designed Monte Carlo computer simulation experiments to empirically test these research hypotheses. Our experimental conditions included seven sample sizes ranging from 50 to 1000, and three distinct model types, each with five specifications ranging from a correct model to the severely misspecified uncorrelated baseline model. In general, we found that for models with small to moderate misspecification, the noncentral chi-square distribution is well approximated when the sample size is large (e.g., greater than 200), but there was evidence of bias in both mean and variance in smaller samples. A key finding was that the test statistics for the uncorrelated variable baseline model did not follow the noncentral chi-square distribution for any model type across any sample size. We discuss the implications of our findings for the SEM fit indices and power estimation procedures that are based on the noncentral chi-square distribution as well as potential directions for future research.},
  eprint = {26824167},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\XX7WKIXD\\Curran e.a. - 2002 - The Noncentral Chi-square Distribution in Misspeci.pdf;C\:\\Users\\User\\Zotero\\storage\\DDRLBYFI\\S15327906MBR3701_01.html},
  number = {1}
}

@book{dingAlgorithm275Computing1990,
  title = {Algorithm {{AS}} 275 {{Computing}} the {{Non}}-Central X2 {{Distribution Function}}},
  author = {Ding, G. and Col, Pos and Dingt, G.},
  date = {1990},
  abstract = {JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Wiley and Royal Statistical Society are collaborating with JSTOR to digitize, preserve and extend access to},
  file = {C\:\\Users\\User\\Zotero\\storage\\IC5UU37L\\Ding e.a. - 1990 - Algorithm AS 275 Computing the Non-central x2 Dist.pdf;C\:\\Users\\User\\Zotero\\storage\\KGISLKS9\\summary.html}
}

@article{EenHoofddoekTe,
  title = {Een hoofddoek is al te veel voor Oeigoeren in China},
  journaltitle = {NRC},
  url = {https://www.nrc.nl/nieuws/2020/02/18/een-hoofddoek-is-al-te-veel-a3990840},
  urldate = {2020-04-13},
  abstract = {Oeigoeren in China: Een lek laat zien dat ook algemene islamitische symbolen reden voor China kunnen zijn om Oeigoeren in kampen op te sluiten.},
  entrysubtype = {newspaper},
  file = {C\:\\Users\\User\\Zotero\\storage\\JTRBJWV4\\een-hoofddoek-is-al-te-veel-a3990840.html},
  langid = {dutch}
}

@article{elad06,
  title = {Comparison of Methodologies to Assess the Convergence of {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard},
  date = {2006-06-20},
  journaltitle = {Computational Statistics \& Data Analysis},
  shortjournal = {Computational Statistics \& Data Analysis},
  volume = {50},
  pages = {2685--2701},
  issn = {0167-9473},
  doi = {10.1016/j.csda.2005.04.018},
  abstract = {One major challenge with the modelization of complex problems using Markov chain Monte Carlo (MCMC) methods is the determination of the length of the chain in order to reach convergence. This paper is devoted to parametric empirical methods testing the stationarity. We compare the methods of Gelman and Rubin, Yu and Mykland, Raftery and Lewis, Geweke, Riemann sums and the subsampling. These methods are tested using three examples: the simple case of the generation of a normal random variable, a bivariate mixture of normal models and a practical case taken from hydrology, namely the shifting level model. Results show that no method works in every case. We therefore suggest a joint use of these techniques. The importance of determining carefully the burn-in period is also highlighted.},
  file = {C\:\\Users\\User\\Zotero\\storage\\BI6QI44D\\El Adlouni e.a. - 2006 - Comparison of methodologies to assess the converge.pdf;C\:\\Users\\User\\Zotero\\storage\\SF4UHRQ2\\S0167947305000836.html},
  keywords = {Burn-in period,Convergence,Gelman and Rubin,Geweke,Gibbs sampler,MCMC,Raftery and Lewis,Riemann sums,Subsampling,Yu and Mykland},
  number = {10}
}

@article{elad06,
  title = {Comparison of Methodologies to Assess the Convergence of Markov Chain Monte Carlo Methods},
  author = {El Adlouni, Salaheddine and Favre, Anne-Catherine and Bobée, Bernard},
  date = {2006-02},
  journaltitle = {Computational Statistics \& Data Analysis},
  volume = {50},
  pages = {2685--2701},
  doi = {10.1016/j.csda.2005.04.018}
}

@book{ende10,
  title = {Applied Missing Data Analysis},
  author = {Enders, Craig K},
  date = {2010},
  publisher = {{Guilford press}},
  file = {C\:\\Users\\User\\Zotero\\storage\\GTM2QHJI\\Enders - 2010 - Applied missing data analysis.pdf},
  isbn = {1-60623-639-3}
}

@article{ende18,
  title = {A Fully Conditional Specification Approach to Multilevel Imputation of Categorical and Continuous Variables.},
  author = {Enders, Craig K. and Keller, Brian T. and Levy, Roy},
  date = {2018-06},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {23},
  pages = {298--317},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000148},
  abstract = {Specialized imputation routines for multilevel data are widely available in software packages, but these methods are generally not equipped to handle a wide range of complexities that are typical of behavioral science data. In particular, existing imputation schemes differ in their ability to handle random slopes, categorical variables, differential relations at level-1 and level-2, and incomplete level-2 variables. Given the limitations of existing imputation tools, the purpose of this manuscript is to describe a flexible imputation approach that can accommodate a diverse set of two-level analysis problems that includes any of the aforementioned features. The procedure employs a fully conditional specification (also known as chained equations) approach with a latent variable formulation for handling incomplete categorical variables. Computer simulations suggest that the proposed procedure works quite well, with trivial biases in most cases. We provide a software program that implements the imputation strategy, and we use an artificial data set to illustrate its use.},
  file = {C\:\\Users\\User\\Zotero\\storage\\J2H75436\\enders-keller--levy-2017--.pdf},
  langid = {english},
  number = {2}
}

@article{EquivalenceTestingPsychological,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  pages = {28},
  file = {C\:\\Users\\User\\Zotero\\storage\\NGADHTNZ\\Equivalence Testing for Psychological Research A .pdf},
  langid = {english}
}

@article{faulPowerFlexibleStatistical2007,
  title = {G*{{Power}} 3: {{A}} Flexible Statistical Power Analysis Program for the Social, Behavioral, and Biomedical Sciences},
  shorttitle = {G*{{Power}} 3},
  author = {Faul, Franz and Erdfelder, Edgar and Lang, Albert-Georg and Buchner, Axel},
  date = {2007-05},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behavior Research Methods},
  volume = {39},
  pages = {175--191},
  issn = {1554-351X, 1554-3528},
  doi = {10.3758/BF03193146},
  file = {C\:\\Users\\User\\Zotero\\storage\\Z7XWPRKC\\Faul e.a. - 2007 - GPower 3 A flexible statistical power analysis p.pdf},
  langid = {english},
  number = {2}
}

@article{fichmanMultipleImputationMissing2003,
  title = {Multiple {{Imputation}} for {{Missing Data}}: {{Making}} the Most of {{What}} You {{Know}}},
  shorttitle = {Multiple {{Imputation}} for {{Missing Data}}},
  author = {Fichman, Mark and Cummings, Jonathon N.},
  date = {2003-07-01},
  journaltitle = {Organizational Research Methods},
  shortjournal = {Organizational Research Methods},
  volume = {6},
  pages = {282--308},
  publisher = {{SAGE Publications Inc}},
  issn = {1094-4281},
  doi = {10.1177/1094428103255532},
  abstract = {Missing data are a common problem in organizational research. Missing data can occur due to attrition in a longitudinal study or nonresponse to questionnaire items in a laboratory or field setting. Improper treatments of missing data (e.g., listwise deletion, mean imputation) can lead to biased statistical inference using complete case analysis statistical techniques. This article presents a simulation and data analysis case study using a method for dealing with missing data, multiple imputation, that allows for valid statistical inference with complete case statistical analysis. Software for implementing multiple imputation under a multivariate normal model is freely and widely available (e.g., NORM, SAS, SOLAS). It should be routinely considered for imputing missing data. The authors illustrate the application of this technique using data from the HomeNet project.},
  file = {C\:\\Users\\User\\Zotero\\storage\\QTIV6ICE\\Fichman en Cummings - 2003 - Multiple Imputation for Missing Data Making the m.pdf},
  number = {3}
}

@online{FullyConditionalSpecification,
  title = {Fully Conditional Specification in Multivariate Imputation: {{Journal}} of {{Statistical Computation}} and {{Simulation}}: {{Vol}} 76, {{No}} 12},
  url = {https://www-tandfonline-com.proxy.library.uu.nl/doi/full/10.1080/10629360600810434},
  urldate = {2020-03-22},
  file = {C\:\\Users\\User\\Zotero\\storage\\F4Y9QAE9\\10629360600810434.html}
}

@online{FundamentalLegalDocuments,
  title = {Fundamental {{Legal Documents}} of {{Communist China}} . {{Edited}} by {{Albert P}}. {{Blaustein}}. ({{South Hackensack}}, {{N}}. {{J}} . : {{Fred B}}. {{Eothman}} \& {{Co}}., 1962. Pp. Xxx, 603. \$14.00.) | {{American Journal}} of {{International Law}} | {{Cambridge Core}}},
  url = {https://www-cambridge-org.proxy.library.uu.nl/core/journals/american-journal-of-international-law/article/fundamental-legal-documents-of-communist-china-edited-by-albert-p-blaustein-south-hackensack-n-j-fred-b-eothman-co-1962-pp-xxx-603-1400/78C64AB1B642A32A2C72C855713B25B0},
  urldate = {2020-04-13},
  file = {C\:\\Users\\User\\Zotero\\storage\\WYMHQ8M2\\78C64AB1B642A32A2C72C855713B25B0.html}
}

@book{gelm13,
  title = {Bayesian {{Data Analysis}}},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  date = {2013},
  publisher = {{CRC Press LLC}},
  location = {{Philadelphia, PA, United States}},
  file = {C\:\\Users\\User\\Zotero\\storage\\ENZRIU2N\\Gelman e.a. - 2013 - Bayesian Data Analysis.pdf;C\:\\Users\\User\\Zotero\\storage\\ZSGD97ZP\\reader.html},
  keywords = {Bayesian statistical decision theory.}
}

@article{gelm92,
  title = {Inference from {{Iterative Simulation Using Multiple Sequences}}},
  author = {Gelman, Andrew and Rubin, Donald B.},
  date = {1992-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {7},
  pages = {457--472},
  doi = {10.1214/ss/1177011136},
  abstract = {The Gibbs sampler, the algorithm of Metropolis and similar iterative simulation methods are potentially very helpful for summarizing multivariate distributions. Used naively, however, iterative simulation can give misleading answers. Our methods are simple and generally applicable to the output of any iterative simulation; they are designed for researchers primarily interested in the science underlying the data and models they are analyzing, rather than for researchers interested in the probability theory underlying the iterative simulations themselves. Our recommended strategy is to use several independent sequences, with starting points sampled from an overdispersed distribution. At each step of the iterative simulation, we obtain, for each univariate estimand of interest, a distributional estimate and an estimate of how much sharper the distributional estimate might become if the simulations were continued indefinitely. Because our focus is on applied inference for Bayesian posterior distributions in real problems, which often tend toward normality after transformations and marginalization, we derive our results as normal-theory approximations to exact Bayesian inference, conditional on the observed simulations. The methods are illustrated on a random-effects mixture model applied to experimental measurements of reaction times of normal and schizophrenic patients.},
  file = {C\:\\Users\\User\\Zotero\\storage\\CWG9QXQK\\Gelman en Rubin - 1992 - Inference from Iterative Simulation Using Multiple.pdf;C\:\\Users\\User\\Zotero\\storage\\EK2UCSZT\\1177011136.html},
  keywords = {Bayesian inference,convergence of stochastic processes,ECM,EM,Gibbs sampler,importance sampling,Metropolis algorithm,multiple imputation,random-effects model,SIR},
  langid = {english},
  number = {4},
  zmnumber = {06853057}
}

@article{gelmanPhilosophyPracticeBayesian2013,
  title = {Philosophy and the Practice of {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  date = {2013},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {66},
  pages = {8--38},
  issn = {2044-8317},
  doi = {10.1111/j.2044-8317.2011.02037.x},
  abstract = {A substantial school in the philosophy of science identifies Bayesian inference with inductive inference and even rationality as such, and seems to be strengthened by the rise and practical success of Bayesian statistics. We argue that the most successful forms of Bayesian statistics do not actually support that particular philosophy but rather accord much better with sophisticated forms of hypothetico-deductivism. We examine the actual role played by prior distributions in Bayesian models, and the crucial aspects of model checking and model revision, which fall outside the scope of Bayesian confirmation theory. We draw on the literature on the consistency of Bayesian updating and also on our experience of applied work in social science. Clarity about these matters should benefit not just philosophy of science, but also statistical practice. At best, the inductivist view has encouraged researchers to fit and compare models without checking them; at worst, theorists have actively discouraged practitioners from performing model checking because it does not fit into their framework.},
  file = {C\:\\Users\\User\\Zotero\\storage\\UQWT97NF\\Gelman en Shalizi - 2013 - Philosophy and the practice of Bayesian statistics.pdf;C\:\\Users\\User\\Zotero\\storage\\JLVQAQE6\\j.2044-8317.2011.02037.html},
  langid = {english},
  number = {1}
}

@online{GenerateMissingValues,
  title = {Generate Missing Values with Ampute},
  url = {https://www.gerkovink.com/Amputation_with_Ampute/Vignette/ampute.html},
  urldate = {2020-03-04},
  file = {C\:\\Users\\User\\Zotero\\storage\\I7RBXQPI\\ampute.html}
}

@online{GeneratingMissingValues,
  title = {Generating Missing Values for Simulation Purposes: A Multivariate Amputation Procedure: {{Journal}} of {{Statistical Computation}} and {{Simulation}}: {{Vol}} 88, {{No}} 15},
  url = {https://www.tandfonline.com/doi/full/10.1080/00949655.2018.1491577},
  urldate = {2020-03-04},
  file = {C\:\\Users\\User\\Zotero\\storage\\J75JCK6X\\00949655.2018.html}
}

@article{gewe92,
  title = {Evaluating the Accuracy of Sampling-Based Approaches to the Calculations of Posterior Moments},
  author = {Geweke, John},
  date = {1992},
  journaltitle = {Bayesian statistics},
  shortjournal = {Bayesian statistics},
  volume = {4},
  pages = {641--649}
}

@article{gewekeBayesianStatistics1992,
  title = {Bayesian Statistics},
  author = {Geweke, John and Bernardo, José M and Berger, James O and Dawid, A Philip and Smith, A},
  date = {1992},
  journaltitle = {Bayesian statistics},
  shortjournal = {Bayesian statistics}
}

@book{gilk95,
  title = {Markov {{Chain Monte Carlo}} in {{Practice}}},
  author = {Gilks, W. R. and Richardson, S. and Spiegelhalter, David},
  date = {1995-12-01},
  publisher = {{CRC Press}},
  abstract = {In a family study of breast cancer, epidemiologists in Southern California increase the power for detecting a gene-environment interaction. In Gambia, a study helps a vaccination program reduce the incidence of Hepatitis B carriage. Archaeologists in Austria place a Bronze Age site in its true temporal location on the calendar scale. And in France, researchers map a rare disease with relatively little variation.Each of these studies applied Markov chain Monte Carlo methods to produce more accurate and inclusive results. General state-space Markov chain theory has seen several developments that have made it both more accessible and more powerful to the general statistician. Markov Chain Monte Carlo in Practice introduces MCMC methods and their applications, providing some theoretical background as well. The authors are researchers who have made key contributions in the recent development of MCMC methodology and its application. Considering the broad audience, the editors emphasize practice rather than theory, keeping the technical content to a minimum. The examples range from the simplest application, Gibbs sampling, to more complex applications. The first chapter contains enough information to allow the reader to start applying MCMC in a basic way. The following chapters cover main issues, important concepts and results, techniques for implementing MCMC, improving its performance, assessing model adequacy, choosing between models, and applications and their domains.Markov Chain Monte Carlo in Practice is a thorough, clear introduction to the methodology and applications of this simple idea with enormous potential. It shows the importance of MCMC in real applications, such as archaeology, astronomy, biostatistics, genetics, epidemiology, and image analysis, and provides an excellent base for MCMC to be applied to other fields as well.},
  eprint = {TRXrMWY_i2IC},
  eprinttype = {googlebooks},
  isbn = {978-0-412-05551-5},
  keywords = {Mathematics / Probability & Statistics / General,Science / Life Sciences / Biology},
  langid = {english},
  pagetotal = {522}
}

@book{grah12,
  title = {Missing Data: {{Analysis}} and Design},
  author = {Graham, John W},
  date = {2012},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\User\\Zotero\\storage\\688F2XST\\Graham - 2012 - Missing data Analysis and design.pdf},
  isbn = {1-4614-4018-1}
}

@online{GraphicalNumericalDiagnostic,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models - {{Bondarenko}} - 2016 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1002/sim.6926},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\KQX8B8SA\\sim.html}
}

@online{GraphicalNumericalDiagnostica,
  title = {Graphical and Numerical Diagnostic Tools to Assess Suitability of Multiple Imputations and Imputation Models - {{Bondarenko}} - 2016 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/epdf/10.1002/sim.6926},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\NHQMIBN7\\sim.html}
}

@article{guoTestingTwoVariances,
  title = {Testing Two Variances for Superiority/Non-Inferiority and Equivalence: {{Using}} the Exhaustion Algorithm for Sample Size Allocation with Cost},
  shorttitle = {Testing Two Variances for Superiority/Non-Inferiority and Equivalence},
  author = {Guo, Jiin-huarng and Luh, Wei-ming},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  volume = {0},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12172},
  abstract = {The equality of two group variances is frequently tested in experiments. However, criticisms of null hypothesis statistical testing on means have recently arisen and there is interest in other types of statistical tests of hypotheses, such as superiority/non-inferiority and equivalence. Although these tests have become more common in psychology and social sciences, the corresponding sample size estimation for these tests is rarely discussed, especially when the sampling unit costs are unequal or group sizes are unequal for two groups. Thus, for finding optimal sample size, the present study derived an initial allocation by approximating the percentiles of an F distribution with the percentiles of the standard normal distribution and used the exhaustion algorithm to select the best combination of group sizes, thereby ensuring the resulting power reaches the designated level and is maximal with a minimal total cost. In this manner, optimization of sample size planning is achieved. The proposed sample size determination has a wide range of applications and is efficient in terms of Type I errors and statistical power in simulations. Finally, an illustrative example from a report by the Health Survey for England, 1995–1997, is presented using hypertension data. For ease of application, four R Shiny apps are provided and benchmarks for setting equivalence margins are suggested.},
  file = {C\:\\Users\\User\\Zotero\\storage\\LVZVTMCR\\Guo en Luh - Testing two variances for superioritynon-inferior.pdf;C\:\\Users\\User\\Zotero\\storage\\KN8JM3YP\\bmsp.html},
  keywords = {hypertension data,optimal allocation,power analysis,sampling cost,Snedecor's F test,variance ratio},
  langid = {english},
  number = {0}
}

@article{harelEstimationAdjustedIncomplete2009,
  title = {The Estimation of {{R}} 2 and Adjusted {{R}} 2 in Incomplete Data Sets Using Multiple Imputation},
  author = {Harel, Ofer},
  date = {2009-10-01},
  journaltitle = {Journal of Applied Statistics},
  volume = {36},
  pages = {1109--1118},
  publisher = {{Taylor \& Francis}},
  issn = {0266-4763},
  doi = {10.1080/02664760802553000},
  abstract = {The coefficient of determination, known also as the R 2, is a common measure in regression analysis. Many scientists use the R 2 and the adjusted R 2 on a regular basis. In most cases, the researchers treat the coefficient of determination as an index of ‘usefulness’ or ‘goodness of fit,’ and in some cases, they even treat it as a model selection tool. In cases in which the data is incomplete, most researchers and common statistical software will use complete case analysis in order to estimate the R 2, a procedure that might lead to biased results. In this paper, I introduce the use of multiple imputation for the estimation of R 2 and adjusted R 2 in incomplete data sets. I illustrate my methodology using a biomedical example.},
  annotation = {\_eprint: https://doi.org/10.1080/02664760802553000},
  file = {C\:\\Users\\User\\Zotero\\storage\\XKMGHWUF\\Harel - 2009 - The estimation of R 2 and adjusted R 2 in incomple.pdf;C\:\\Users\\User\\Zotero\\storage\\9GZA2JEG\\02664760802553000.html},
  keywords = {coefficient of determination,incomplete data,linear regression,multiple imputation},
  number = {10}
}

@article{hazardFundamentalLegalDocuments1963,
  title = {Fundamental {{Legal Documents}} of {{Communist China}} . {{Edited}} by {{Albert P}}. {{Blaustein}}. ({{South Hackensack}}, {{N}}. {{J}} . : {{Fred B}}. {{Eothman}} \& {{Co}}., 1962. Pp. Xxx, 603. \$14.00.)},
  shorttitle = {Fundamental {{Legal Documents}} of {{Communist China}} . {{Edited}} by {{Albert P}}. {{Blaustein}}. ({{South Hackensack}}, {{N}}. {{J}} .},
  author = {Hazard, John N.},
  date = {1963-04},
  journaltitle = {American Journal of International Law},
  volume = {57},
  pages = {471--472},
  publisher = {{Cambridge University Press}},
  issn = {0002-9300, 2161-7953},
  doi = {10.2307/2196019},
  abstract = {//static.cambridge.org/content/id/urn\%3Acambridge.org\%3Aid\%3Aarticle\%3AS0002930000129963/resource/name/firstPage-S0002930000129963a.jpg},
  file = {C\:\\Users\\User\\Zotero\\storage\\4SQW7RZV\\Hazard - 1963 - Fundamental Legal Documents of Communist China . E.pdf;C\:\\Users\\User\\Zotero\\storage\\DHK85BAR\\78C64AB1B642A32A2C72C855713B25B0.html},
  langid = {english},
  number = {2}
}

@article{hewittNoteComputingChisquare1988,
  title = {A Note on Computing the Chi-Square Noncentrality Parameter for Power Analyses},
  author = {Hewitt, J. K. and Heath, A. C.},
  date = {1988-01-01},
  journaltitle = {Behavior Genetics},
  shortjournal = {Behav Genet},
  volume = {18},
  pages = {105--108},
  issn = {1573-3297},
  doi = {10.1007/BF01067079},
  abstract = {Power calculations for a variety of research designs used in behavior genetics require the determination of a chi-square noncentrality parameter. For many purposes published tables are adequate, but for cases where they are not we provide a graph for approximating the required parameters for up to 300 degrees of freedom and illustrate a straightforward procedure for accurate determination using a widely available SAS function.},
  file = {C\:\\Users\\User\\Zotero\\storage\\YLS25CMC\\Hewitt en Heath - 1988 - A note on computing the chi-square noncentrality p.pdf},
  keywords = {noncentral chi-square,power calculation},
  langid = {english},
  number = {1}
}

@article{higginsMeasuringInconsistencyMetaanalyses2003,
  title = {Measuring Inconsistency in Meta-Analyses},
  author = {Higgins, Julian P T and Thompson, Simon G and Deeks, Jonathan J and Altman, Douglas G},
  date = {2003-09-06},
  journaltitle = {BMJ : British Medical Journal},
  shortjournal = {BMJ},
  volume = {327},
  pages = {557--560},
  issn = {0959-8138},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC192859/},
  urldate = {2019-06-13},
  abstract = {Cochrane Reviews have recently started including the quantity I2 to help readers assess the consistency of the results of studies in meta-analyses. What does this new quantity mean, and why is assessment of heterogeneity so important to clinical practice?},
  eprint = {12958120},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\V75XI7ZS\\Higgins et al. - 2003 - Measuring inconsistency in meta-analyses.pdf},
  number = {7414},
  pmcid = {PMC192859}
}

@article{higginsQuantifyingHeterogeneityMetaanalysis2002,
  title = {Quantifying Heterogeneity in a Meta-Analysis},
  author = {Higgins, Julian P. T. and Thompson, Simon G.},
  date = {2002-06-15},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {21},
  pages = {1539--1558},
  issn = {0277-6715},
  doi = {10.1002/sim.1186},
  abstract = {The extent of heterogeneity in a meta-analysis partly determines the difficulty in drawing overall conclusions. This extent may be measured by estimating a between-study variance, but interpretation is then specific to a particular treatment effect metric. A test for the existence of heterogeneity exists, but depends on the number of studies in the meta-analysis. We develop measures of the impact of heterogeneity on a meta-analysis, from mathematical criteria, that are independent of the number of studies and the treatment effect metric. We derive and propose three suitable statistics: H is the square root of the chi2 heterogeneity statistic divided by its degrees of freedom; R is the ratio of the standard error of the underlying mean from a random effects meta-analysis to the standard error of a fixed effect meta-analytic estimate, and I2 is a transformation of (H) that describes the proportion of total variation in study estimates that is due to heterogeneity. We discuss interpretation, interval estimates and other properties of these measures and examine them in five example data sets showing different amounts of heterogeneity. We conclude that H and I2, which can usually be calculated for published meta-analyses, are particularly useful summaries of the impact of heterogeneity. One or both should be presented in published meta-analyses in preference to the test for heterogeneity.},
  eprint = {12111919},
  eprinttype = {pmid},
  keywords = {Albumins,Chemotherapy; Adjuvant,Clinical Trials as Topic,Cognition Disorders,Cytidine Diphosphate Choline,Fibrosis,Fracture Fixation,Hip Fractures,Humans,Meta-Analysis as Topic,Resuscitation,Sarcoma,Sclerotherapy,Statistics as Topic},
  langid = {english},
  number = {11}
}

@book{hoff09,
  title = {A {{First Course}} in {{Bayesian Statistical Methods}}},
  author = {Hoff, Peter D.},
  date = {2009},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-0-387-92407-6},
  file = {C\:\\Users\\User\\Zotero\\storage\\YPEBD5NA\\Hoff - 2009 - A First Course in Bayesian Statistical Methods.pdf},
  series = {Springer {{Texts}} in {{Statistics}}}
}

@article{hortonMultipleImputationPractice2001,
  title = {Multiple {{Imputation}} in {{Practice}}: {{Comparison}} of {{Software Packages}} for {{Regression Models With Missing Variables}}},
  shorttitle = {Multiple {{Imputation}} in {{Practice}}},
  author = {Horton, Nicholas J and Lipsitz, Stuart R},
  date = {2001-08},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {55},
  pages = {244--254},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313001317098266},
  file = {C\:\\Users\\User\\Zotero\\storage\\RCIXUG22\\Horton en Lipsitz - 2001 - Multiple Imputation in Practice Comparison of Sof.pdf},
  langid = {english},
  number = {3}
}

@article{huedo-medinaAssessingHeterogeneityMetaanalysis2006,
  title = {Assessing Heterogeneity in Meta-Analysis: {{Q}} Statistic or {{I}}² Index?},
  shorttitle = {Assessing Heterogeneity in Meta-Analysis},
  author = {Huedo-Medina, Tania B. and Sánchez-Meca, Julio and Marín-Martínez, Fulgencio and Botella, Juan},
  date = {2006},
  journaltitle = {Psychological Methods},
  shortjournal = {Psychological Methods},
  volume = {11},
  pages = {193--206},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.11.2.193},
  abstract = {In meta-analysis, the usual way of assessing whether a set of single studies are homogeneous is by means of the Q test. However, the Q test only informs us about the presence versus the absence of heterogeneity, but it does not report on the extent of such heterogeneity. Recently, the I2 index has been proposed to quantify the degree of heterogeneity in a meta-analysis. In this paper, the performances of the Q test and the confidence interval around the I2 index are compared by means of a Monte Carlo simulation. The results show the utility of the I2 index as a complement to the Q test, although it has the same problems of power with a small number of studies.},
  file = {C\:\\Users\\User\\Zotero\\storage\\AWAQ8LAZ\\Huedo-Medina e.a. - 2006 - Assessing heterogeneity in meta-analysis Q statis.pdf},
  langid = {english},
  number = {2}
}

@book{hynd18,
  title = {Forecasting: Principles and Practice},
  shorttitle = {Forecasting},
  author = {Hyndman, Rob J. and Athanasopoulos, George},
  date = {2018},
  publisher = {{OTexts}},
  file = {C\:\\Users\\User\\Zotero\\storage\\NISHZF9I\\Hyndman en Athanasopoulos - 2018 - Forecasting principles and practice.pdf;C\:\\Users\\User\\Zotero\\storage\\LTZRLHJ6\\books.html}
}

@book{Jackman:2015,
  title = {: Classes and Methods for  Developed in the Political Science Computational Laboratory, Stanford University},
  author = {Jackman, Simon},
  date = {2015},
  url = {https://CRAN.R-project.org/package=pscl},
  annotation = {package version 1.4.9}
}

@article{janSampleSizeDeterminations2014,
  title = {Sample Size Determinations for {{Welch}}'s Test in One-Way Heteroscedastic {{ANOVA}}},
  author = {Jan, Show-Li and Shieh, Gwowen},
  date = {2014-02},
  journaltitle = {The British Journal of Mathematical and Statistical Psychology},
  shortjournal = {Br J Math Stat Psychol},
  volume = {67},
  pages = {72--93},
  issn = {2044-8317},
  doi = {10.1111/bmsp.12006},
  abstract = {For one-way fixed effects ANOVA, it is well known that the conventional F test of the equality of means is not robust to unequal variances, and numerous methods have been proposed for dealing with heteroscedasticity. On the basis of extensive empirical evidence of Type I error control and power performance, Welch's procedure is frequently recommended as the major alternative to the ANOVA F test under variance heterogeneity. To enhance its practical usefulness, this paper considers an important aspect of Welch's method in determining the sample size necessary to achieve a given power. Simulation studies are conducted to compare two approximate power functions of Welch's test for their accuracy in sample size calculations over a wide variety of model configurations with heteroscedastic structures. The numerical investigations show that Levy's (1978a) approach is clearly more accurate than the formula of Luh and Guo (2011) for the range of model specifications considered here. Accordingly, computer programs are provided to implement the technique recommended by Levy for power calculation and sample size determination within the context of the one-way heteroscedastic ANOVA model.},
  eprint = {23316952},
  eprinttype = {pmid},
  keywords = {Analysis of Variance,Cost Allocation,Humans,Models; Statistical,Research Design,Sample Size,Software},
  langid = {english},
  number = {1}
}

@article{janSampleSizeDeterminations2014a,
  title = {Sample Size Determinations for {{Welch}}'s Test in One-Way Heteroscedastic {{ANOVA}}},
  author = {Jan, Show-Li and Shieh, Gwowen},
  date = {2014-02-01},
  journaltitle = {British Journal of Mathematical and Statistical Psychology},
  shortjournal = {British Journal of Mathematical and Statistical Psychology},
  volume = {67},
  pages = {72--93},
  issn = {0007-1102},
  doi = {10.1111/bmsp.12006},
  abstract = {For one-way fixed effects ANOVA, it is well known that the conventional F test of the equality of means is not robust to unequal variances, and numerous methods have been proposed for dealing with heteroscedasticity. On the basis of extensive empirical evidence of Type I error control and power performance, Welch's procedure is frequently recommended as the major alternative to the ANOVA F test under variance heterogeneity. To enhance its practical usefulness, this paper considers an important aspect of Welch's method in determining the sample size necessary to achieve a given power. Simulation studies are conducted to compare two approximate power functions of Welch's test for their accuracy in sample size calculations over a wide variety of model configurations with heteroscedastic structures. The numerical investigations show that Levy's (1978a) approach is clearly more accurate than the formula of Luh and Guo (2011) for the range of model specifications considered here. Accordingly, computer programs are provided to implement the technique recommended by Levy for power calculation and sample size determination within the context of the one-way heteroscedastic ANOVA model.},
  file = {C\:\\Users\\User\\Zotero\\storage\\V2S7GYC2\\Jan en Shieh - 2014 - Sample size determinations for Welch's test in one.pdf;C\:\\Users\\User\\Zotero\\storage\\B5E3ASHB\\bmsp.html},
  number = {1}
}

@article{kelleyConfidenceIntervalsStandardized2007,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  date = {2007-02-22},
  journaltitle = {Journal of Statistical Software},
  volume = {20},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v020.i08},
  file = {C\:\\Users\\User\\Zotero\\storage\\MCEE7LJB\\Kelley - 2007 - Confidence Intervals for Standardized Effect Sizes.pdf;C\:\\Users\\User\\Zotero\\storage\\7KHZJRI8\\v020i08.html},
  langid = {english},
  number = {1}
}

@article{kelleyConfidenceIntervalsStandardized2007a,
  title = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}: {{Theory}}, {{Application}}, and {{Implementation}}},
  shorttitle = {Confidence {{Intervals}} for {{Standardized Effect Sizes}}},
  author = {Kelley, Ken},
  date = {2007-02-22},
  journaltitle = {Journal of Statistical Software},
  volume = {20},
  pages = {1--24},
  issn = {1548-7660},
  doi = {10.18637/jss.v020.i08},
  file = {C\:\\Users\\User\\Zotero\\storage\\QDZH3HNW\\Kelley - 2007 - Confidence Intervals for Standardized Effect Sizes.pdf;C\:\\Users\\User\\Zotero\\storage\\IZAQCCDB\\v020i08.html},
  langid = {english},
  number = {1}
}

@article{kesterRapportTienduizendenOeigoeren2020,
  title = {Rapport: tienduizenden Oeigoeren werken onder dwang voor internationale merken},
  shorttitle = {Rapport},
  author = {Kester, Sacha},
  date = {2020-03-02T08:12+01:00},
  journaltitle = {de Volkskrant},
  url = {https://www.volkskrant.nl/gs-bdee7e1f},
  urldate = {2020-04-13},
  abstract = {Zeker 80 duizend Oeigoeren zijn vanuit de Chinese provincie Xinjiang naar fabrieken in andere delen van het land gebracht, om onder dwang te werken...},
  entrysubtype = {newspaper},
  file = {C\:\\Users\\User\\Zotero\\storage\\T7SYS9AW\\rapport-tienduizenden-oeigoeren-werken-onder-dwang-voor-internationale-merken~bdee7e1f.html},
  journalsubtitle = {Nieuws \& Achtergrond},
  langid = {dutch}
}

@article{kosi13,
  title = {Private Traits and Attributes Are Predictable from Digital Records of Human Behavior},
  author = {Kosinski, Michal and Stillwell, David and Graepel, Thore},
  date = {2013-04-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {110},
  pages = {5802--5805},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1218772110},
  abstract = {We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88\% of cases, African Americans and Caucasian Americans in 95\% of cases, and between Democrat and Republican in 85\% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.},
  eprint = {23479631},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\F73ABZHL\\Kosinski e.a. - 2013 - Private traits and attributes are predictable from.pdf;C\:\\Users\\User\\Zotero\\storage\\K93WL2T5\\5802.html},
  keywords = {big data,computational social science,data mining,machine learning,psychological assessment,social networks},
  langid = {english},
  number = {15}
}

@online{kosi18,
  title = {{{myPersonality}}.Org},
  author = {Kosinski, Michal},
  date = {2018},
  url = {https://sites.google.com/michalkosinski.com/mypersonality},
  urldate = {2020-04-07},
  file = {C\:\\Users\\User\\Zotero\\storage\\J2AYTEDC\\mypersonality.html},
  langid = {english}
}

@article{kosinskiPrivateTraitsAttributes2013a,
  title = {Private Traits and Attributes Are Predictable from Digital Records of Human Behavior},
  author = {Kosinski, M. and Stillwell, D. and Graepel, T.},
  date = {2013-04-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  pages = {5802--5805},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1218772110},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZUMSVXIH\\Kosinski e.a. - 2013 - Private traits and attributes are predictable from.pdf},
  langid = {english},
  number = {15}
}

@article{kosinskiPrivateTraitsAttributes2013b,
  title = {Private Traits and Attributes Are Predictable from Digital Records of Human Behavior},
  author = {Kosinski, M. and Stillwell, D. and Graepel, T.},
  date = {2013-04-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  pages = {5802--5805},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1218772110},
  file = {C\:\\Users\\User\\Zotero\\storage\\I6Z34IPY\\Kosinski e.a. - 2013 - Private traits and attributes are predictable from.pdf},
  langid = {english},
  number = {15}
}

@article{kosinskiPrivateTraitsAttributes2013c,
  title = {Private Traits and Attributes Are Predictable from Digital Records of Human Behavior},
  author = {Kosinski, Michal and Stillwell, David and Graepel, Thore},
  date = {2013-04-09},
  journaltitle = {Proceedings of the National Academy of Sciences},
  shortjournal = {PNAS},
  volume = {110},
  pages = {5802--5805},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1218772110},
  abstract = {We show that easily accessible digital records of behavior, Facebook Likes, can be used to automatically and accurately predict a range of highly sensitive personal attributes including: sexual orientation, ethnicity, religious and political views, personality traits, intelligence, happiness, use of addictive substances, parental separation, age, and gender. The analysis presented is based on a dataset of over 58,000 volunteers who provided their Facebook Likes, detailed demographic profiles, and the results of several psychometric tests. The proposed model uses dimensionality reduction for preprocessing the Likes data, which are then entered into logistic/linear regression to predict individual psychodemographic profiles from Likes. The model correctly discriminates between homosexual and heterosexual men in 88\% of cases, African Americans and Caucasian Americans in 95\% of cases, and between Democrat and Republican in 85\% of cases. For the personality trait “Openness,” prediction accuracy is close to the test–retest accuracy of a standard personality test. We give examples of associations between attributes and Likes and discuss implications for online personalization and privacy.},
  eprint = {23479631},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\DR6K2EV9\\Kosinski e.a. - 2013 - Private traits and attributes are predictable from.pdf;C\:\\Users\\User\\Zotero\\storage\\DZWFTQH4\\5802.html},
  keywords = {big data,computational social science,data mining,machine learning,psychological assessment,social networks},
  langid = {english},
  number = {15}
}

@report{lace07,
  title = {Sequential Regression Multiple Imputation for Incomplete Multivariate Data Using {{Markov}} Chain {{Monte Carlo}}},
  author = {Lacerda, Miguel and Ardington, Cally and Leibbrandt, Murray},
  date = {2007},
  institution = {{University of Cape Town, South Africa}},
  file = {C\:\\Users\\User\\Zotero\\storage\\NIKK7HSY\\2008_13.pdf}
}

@article{lakensEquivalenceTestsPractical2017,
  title = {Equivalence {{Tests}}: {{A Practical Primer}} for t {{Tests}}, {{Correlations}}, and {{Meta}}-{{Analyses}}},
  shorttitle = {Equivalence {{Tests}}},
  author = {Lakens, Daniël},
  date = {2017-05-01},
  journaltitle = {Social Psychological and Personality Science},
  shortjournal = {Social Psychological and Personality Science},
  volume = {8},
  pages = {355--362},
  issn = {1948-5506},
  doi = {10.1177/1948550617697177},
  abstract = {Scientists should be able to provide support for the absence of a meaningful effect. Currently, researchers often incorrectly conclude an effect is absent based a nonsignificant result. A widely recommended approach within a frequentist framework is to test for equivalence. In equivalence tests, such as the two one-sided tests (TOST) procedure discussed in this article, an upper and lower equivalence bound is specified based on the smallest effect size of interest. The TOST procedure can be used to statistically reject the presence of effects large enough to be considered worthwhile. This practical primer with accompanying spreadsheet and R package enables psychologists to easily perform equivalence tests (and power analyses) by setting equivalence bounds based on standardized effect sizes and provides recommendations to prespecify equivalence bounds. Extending your statistical tool kit with equivalence tests is an easy way to improve your statistical and theoretical inferences.},
  file = {C\:\\Users\\User\\Zotero\\storage\\WVRV3QIE\\Lakens - 2017 - Equivalence Tests A Practical Primer for t Tests,.pdf},
  langid = {english},
  number = {4}
}

@article{lambertRobustMCMCConvergence2020,
  title = {\${{R}}\^*\$: {{A}} Robust {{MCMC}} Convergence Diagnostic with Uncertainty Using Gradient-Boosted Machines},
  shorttitle = {\${{R}}\^*\$},
  author = {Lambert, Ben and Vehtari, Aki},
  date = {2020-09-01},
  url = {http://arxiv.org/abs/2003.07900},
  urldate = {2020-09-03},
  abstract = {Markov chain Monte Carlo (MCMC) has transformed Bayesian model inference over the past three decades: mainly because of this, Bayesian inference is now a workhorse of applied scientists. Under general conditions, MCMC sampling converges asymptotically to the posterior distribution, but this provides no guarantees about its finite sample performance. The predominant method for monitoring convergence is to run multiple chains and monitor individual chains' characteristics and compare these to the population as a whole: if within-chain and between-chain summaries are comparable, then this is taken to indicate that the chains have converged to a common stationary distribution. Here, we introduce a new method for diagnosing convergence based on whether a machine learning classifier model can successfully discriminate the individual chains. We call this convergence measure \$R\^*\$. In contrast to the predominant \$\textbackslash widehat\{R\}\$, \$R\^*\$ is a single statistic across all parameters that indicates lack of mixing, although individual variables' importance for this metric can also be determined. Additionally, \$R\^*\$ is not based on any single characteristic of the sampling distribution; instead using all the information in the chain, including that given by the joint sampling distribution, which is currently largely overlooked by existing approaches. Since our choice of machine learning classifier, a gradient-boosted regression trees model (GBM), provides uncertainty in predictions, as a byproduct, we obtain uncertainty in \$R\^*\$. The method is straightforward to implement, robust to GBM hyperparameter choice, and could be a complementary additional check on MCMC convergence for applied analyses.},
  archivePrefix = {arXiv},
  eprint = {2003.07900},
  eprinttype = {arxiv},
  file = {C\:\\Users\\User\\Zotero\\storage\\25RYXYQ6\\Lambert en Vehtari - 2020 - $R^$ A robust MCMC convergence diagnostic with u.pdf;C\:\\Users\\User\\Zotero\\storage\\JTMX9LDE\\2003.html},
  keywords = {Statistics - Applications,Statistics - Methodology},
  primaryClass = {stat}
}

@online{lavaan,
  title = {The Lavaan {{Project}}},
  url = {https://lavaan.ugent.be/tutorial/est.html},
  urldate = {2020-06-26},
  file = {C\:\\Users\\User\\Zotero\\storage\\LPLS5WRX\\est.html}
}

@article{li14,
  title = {Multiple {{Imputation}} by {{Ordered Monotone Blocks With Application}} to the {{Anthrax Vaccine Research Program}}},
  author = {Li, Fan and Baccini, Michela and Mealli, Fabrizia and Zell, Elizabeth R. and Frangakis, Constantine E. and Rubin, Donald B.},
  date = {2014-07-03},
  journaltitle = {Journal of Computational and Graphical Statistics},
  shortjournal = {Journal of Computational and Graphical Statistics},
  volume = {23},
  pages = {877--892},
  issn = {1061-8600},
  doi = {10.1080/10618600.2013.826583},
  abstract = {Multiple imputation (MI) has become a standard statistical technique for dealing with missing values. The CDC Anthrax Vaccine Research Program (AVRP) dataset created new challenges for MI due to the large number of variables of different types and the limited sample size. A common method for imputing missing data in such complex studies is to specify, for each of J variables with missing values, a univariate conditional distribution given all other variables, and then to draw imputations by iterating over the J conditional distributions. Such fully conditional imputation strategies have the theoretical drawback that the conditional distributions may be incompatible. When the missingness pattern is monotone, a theoretically valid approach is to specify, for each variable with missing values, a conditional distribution given the variables with fewer or the same number of missing values and sequentially draw from these distributions. In this article, we propose the “multiple imputation by ordered monotone blocks” approach, which combines these two basic approaches by decomposing any missingness pattern into a collection of smaller “constructed” monotone missingness patterns, and iterating. We apply this strategy to impute the missing data in the AVRP interim data. Supplemental materials, including all source code and a synthetic example dataset, are available online.},
  file = {C\:\\Users\\User\\Zotero\\storage\\Z2SXCLDC\\Li e.a. - 2014 - Multiple Imputation by Ordered Monotone Blocks Wit.pdf;C\:\\Users\\User\\Zotero\\storage\\PUZN3HTY\\10618600.2013.html},
  number = {3}
}

@article{li91,
  title = {Significance Levels from Repeated P-Values with Multiply-Imputed Data},
  author = {Li, Kim-Hung and Meng, Xiao-Li and Raghunathan, Trivellore E and Rubin, Donald B},
  date = {1991},
  journaltitle = {Statistica Sinica},
  shortjournal = {Statistica Sinica},
  pages = {65--92},
  issn = {1017-0405},
  file = {C\:\\Users\\User\\Zotero\\storage\\3KXYTRMF\\Li Meng Raghunathan Rubin - combining p values.pdf}
}

@article{liu14,
  title = {On the Stationary Distribution of Iterative Imputations},
  author = {Liu, J. and Gelman, A. and Hill, J. and Su, Y.-S. and Kropko, J.},
  date = {2014-03-01},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {101},
  pages = {155--173},
  issn = {0006-3444, 1464-3510},
  doi = {10.1093/biomet/ast044},
  abstract = {Iterative imputation, in which variables are imputed one at a time conditional on all the oth- 15 ers, is a popular technique that can be convenient and flexible, as it replaces a potentially difficult multivariate modeling problem with relatively simple univariate regressions. In this paper, we begin to characterize the stationary distributions of iterative imputations and their statistical properties, accounting for the conditional models being iteratively estimated from data rather than being pre-specified. When the families of conditional models are compatible, we provide 20 sufficient conditions under which the imputation distribution converges in total variation to the posterior distribution of a Bayesian model. When the conditional models are incompatible but valid, we show that the combined imputation estimator is consistent.},
  file = {C\:\\Users\\User\\Zotero\\storage\\5BFL93ME\\Liu e.a. - 2014 - On the stationary distribution of iterative imputa.pdf},
  langid = {english},
  number = {1}
}

@article{liuStationaryDistributionIterative2013,
  title = {On the Stationary Distribution of Iterative Imputations},
  author = {Liu, Jingchen and Gelman, Andrew and Hill, Jennifer and Su, Yu-Sung and Kropko, Jonathan},
  date = {2013},
  journaltitle = {Biometrika},
  shortjournal = {Biometrika},
  volume = {101},
  pages = {155--173},
  issn = {1464-3510},
  number = {1}
}

@article{loMachineLearningStatistical2019,
  title = {Machine {{Learning}} with {{Statistical Imputation}} for {{Predicting Drug Approvals}}},
  author = {Lo, Andrew W. and Siah, Kien Wei and Wong, Chi Heem},
  year = {Thu May 30 2019 16:32:31 GMT+0000 (Coordinated Universal Time)},
  doi = {10.1162/99608f92.5c5f0525},
  langid = {english}
}

@software{luiLangidPy2020,
  title = {Langid.Py},
  shorttitle = {Langid},
  author = {Lui, Marco},
  date = {2020-04-06},
  url = {https://pypi.org/project/langid/},
  urldate = {2020-04-07},
  abstract = {langid.py is a standalone Language Identification (LangID) tool},
  keywords = {HighPerformanceComputing,ModelDeployment,NumericalMathematics},
  version = {1.1.6}
}

@book{lync07,
  title = {Introduction to Applied {{Bayesian}} Statistics and Estimation for Social Scientists},
  author = {Lynch, Scott M.},
  date = {2007},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\User\\Zotero\\storage\\GD5UTHAC\\books.html}
}

@book{mack03,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David JC and Mac Kay, David JC},
  date = {2003},
  publisher = {{Cambridge university press}}
}

@book{McCullagh+Nelder:1989,
  title = {Generalized Linear Models},
  author = {McCullagh, Peter and Nelder, John A.},
  date = {1989},
  edition = {2},
  publisher = {{Chapman \& Hall}},
  location = {{London}},
  doi = {10.1007/978-1-4899-3242-6}
}

@article{meehlTheoryTestingPsychologyPhysics1967,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  date = {1967-06},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philosophy of Science},
  volume = {34},
  pages = {103--115},
  issn = {0031-8248, 1539-767X},
  doi = {10.1086/288135},
  file = {C\:\\Users\\User\\Zotero\\storage\\YH55TTS3\\Meehl - 1967 - Theory-Testing in Psychology and Physics A Method.pdf},
  langid = {english},
  number = {2}
}

@article{meehlTheoryTestingPsychologyPhysics1967a,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  date = {1967-06-01},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philosophy of Science},
  volume = {34},
  pages = {103--115},
  issn = {0031-8248},
  doi = {10.1086/288135},
  abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation.},
  file = {C\:\\Users\\User\\Zotero\\storage\\96YPSH83\\Meehl - 1967 - Theory-Testing in Psychology and Physics A Method.pdf;C\:\\Users\\User\\Zotero\\storage\\SNBSFY4L\\288135.html},
  number = {2}
}

@article{meng94,
  title = {Multiple-{{Imputation Inferences}} with {{Uncongenial Sources}} of {{Input}}},
  author = {Meng, Xiao-Li},
  date = {1994-11},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {9},
  pages = {538--558},
  issn = {0883-4237, 2168-8745},
  doi = {10.1214/ss/1177010269},
  abstract = {Conducting sample surveys, imputing incomplete observations, and analyzing the resulting data are three indispensable phases of modern practice with public-use data files and with many other statistical applications. Each phase inherits different input, including the information preceding it and the intellectual assessments available, and aims to provide output that is one step closer to arriving at statistical inferences with scientific relevance. However, the role of the imputation phase has often been viewed as merely providing computational convenience for users of data. Although facilitating computation is very important, such a viewpoint ignores the imputer's assessments and information inaccessible to the users. This view underlies the recent controversy over the validity of multiple-imputation inference when a procedure for analyzing multiply imputed data sets cannot be derived from (is "uncongenial" to) the model adopted for multiple imputation. Given sensible imputations and complete-data analysis procedures, inferences from standard multiple-imputation combining rules are typically superior to, and thus different from, users' incomplete-data analyses. The latter may suffer from serious nonresponse biases because such analyses often must rely on convenient but unrealistic assumptions about the nonresponse mechanism. When it is desirable to conduct inferences under models for nonresponse other than the original imputation model, a possible alternative to recreating imputations is to incorporate appropriate importance weights into the standard combining rules. These points are reviewed and explored by simple examples and general theory, from both Bayesian and frequentist perspectives, particularly from the randomization perspective. Some convenient terms are suggested for facilitating communication among researchers from different perspectives when evaluating multiple-imputation inferences with uncongenial sources of input.},
  file = {C\:\\Users\\User\\Zotero\\storage\\PHA35LD7\\Meng - 1994 - Multiple-Imputation Inferences with Uncongenial So.pdf;C\:\\Users\\User\\Zotero\\storage\\MNNUQRPL\\1177010269.html},
  keywords = {Congeniality,importance sampling,incomplete data,missing data,nonresponse,normalizing constants,public-use data file,randomization,self-efficiency},
  langid = {english},
  number = {4}
}

@article{meynersEquivalenceTestsReview2012,
  title = {Equivalence Tests – {{A}} Review},
  author = {Meyners, Michael},
  date = {2012-12},
  journaltitle = {Food Quality and Preference},
  shortjournal = {Food Quality and Preference},
  volume = {26},
  pages = {231--245},
  issn = {09503293},
  doi = {10.1016/j.foodqual.2012.05.003},
  file = {C\:\\Users\\User\\Zotero\\storage\\XFJJ4CPE\\Meyners - 2012 - Equivalence tests – A review.pdf},
  langid = {english},
  number = {2}
}

@article{mice,
  title = {Mice: {{Multivariate Imputation}} by {{Chained Equations}} in {{R}}},
  shorttitle = {Mice},
  author = {Van Buuren, Stef and Groothuis-Oudshoorn, Karin},
  date = {2011-12-12},
  journaltitle = {Journal of Statistical Software},
  volume = {45},
  pages = {1--67},
  doi = {10.18637/jss.v045.i03},
  file = {C\:\\Users\\User\\Zotero\\storage\\W7DXJIVB\\Buuren en Groothuis-Oudshoorn - 2011 - mice Multivariate Imputation by Chained Equations.pdf;C\:\\Users\\User\\Zotero\\storage\\6GARXLG6\\v045i03.html},
  langid = {english},
  number = {1}
}

@online{ModelCheckingMultiple,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study | {{Emerging Themes}} in {{Epidemiology}} | {{Full Text}}},
  url = {https://ete-online.biomedcentral.com/articles/10.1186/s12982-017-0062-6},
  urldate = {2019-09-27},
  file = {C\:\\Users\\User\\Zotero\\storage\\W4U8K8EK\\s12982-017-0062-6.html}
}

@article{morr19,
  title = {Using Simulation Studies to Evaluate Statistical Methods},
  author = {Morris, Tim P. and White, Ian R. and Crowther, Michael J.},
  date = {2019-05-20},
  journaltitle = {Statistics in Medicine},
  volume = {38},
  pages = {2074--2102},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1097-0258},
  doi = {10.1002/sim.8086},
  abstract = {Simulation studies are computer experiments that involve creating data by pseudo‐random sampling. A key strength of simulation studies is the ability to understand the behavior of statistical methods...},
  file = {C\:\\Users\\User\\Zotero\\storage\\C2X36276\\Morris e.a. - 2019 - Using simulation studies to evaluate statistical m.pdf;C\:\\Users\\User\\Zotero\\storage\\LN3P265W\\login.html},
  langid = {english},
  number = {11}
}

@article{Mullahy:1986,
  title = {Specification and Testing of Some Modified Count Data Models},
  author = {Mullahy, John},
  date = {1986},
  journaltitle = {Journal of Econometrics},
  volume = {33},
  pages = {341--365},
  doi = {10.1016/0304-4076(86)90002-3},
  number = {3}
}

@article{murr18,
  title = {Multiple {{Imputation}}: {{A Review}} of {{Practical}} and {{Theoretical Findings}}},
  shorttitle = {Multiple {{Imputation}}},
  author = {Murray, Jared S.},
  date = {2018-05},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {33},
  pages = {142--159},
  doi = {10.1214/18-STS644},
  abstract = {Multiple imputation is a straightforward method for handling missing data in a principled fashion. This paper presents an overview of multiple imputation, including important theoretical results and their practical implications for generating and using multiple imputations. A review of strategies for generating imputations follows, including recent developments in flexible joint modeling and sequential regression/chained equations/fully conditional specification approaches. Finally, we compare and contrast different methods for generating imputations on a range of criteria before identifying promising avenues for future research.},
  file = {C\:\\Users\\User\\Zotero\\storage\\TT9QUVIH\\Murray - 2018 - Multiple Imputation A Review of Practical and The.pdf},
  langid = {english},
  number = {2}
}

@book{mvtnorm,
  title = {Computation of Multivariate Normal and t Probabilities},
  author = {Genz, Alan and Bretz, Frank},
  date = {2009},
  publisher = {{Springer-Verlag}},
  location = {{Heidelberg}},
  isbn = {978-3-642-01688-2},
  series = {Lecture Notes in Statistics}
}

@article{neym34,
  title = {On the {{Two Different Aspects}} of the {{Representative Method}}: {{The Method}} of {{Stratified Sampling}} and the {{Method}} of {{Purposive Selection}}},
  author = {Neyman, Jerzy},
  date = {1934},
  journaltitle = {Journal of the Royal Statistical Society},
  volume = {97},
  pages = {558--625},
  doi = {10.2307/2342192},
  number = {4}
}

@article{nguy17,
  title = {Model Checking in Multiple Imputation: An Overview and Case Study},
  shorttitle = {Model Checking in Multiple Imputation},
  author = {Nguyen, Cattram D. and Carlin, John B. and Lee, Katherine J.},
  date = {2017-08-23},
  journaltitle = {Emerging Themes in Epidemiology},
  shortjournal = {Emerging Themes in Epidemiology},
  volume = {14},
  pages = {8},
  issn = {1742-7622},
  doi = {10.1186/s12982-017-0062-6},
  abstract = {Multiple imputation has become very popular as a general-purpose method for handling missing data. The validity of multiple-imputation-based analyses relies on the use of an appropriate model to impute the missing values. Despite the widespread use of multiple imputation, there are few guidelines available for checking imputation models.},
  file = {C\:\\Users\\User\\Zotero\\storage\\GNUQEAQA\\Nguyen e.a. - 2017 - Model checking in multiple imputation an overview.pdf;C\:\\Users\\User\\Zotero\\storage\\CDXNKIZY\\s12982-017-0062-6.html},
  number = {1}
}

@online{NoncentralityParameterOverview,
  title = {Noncentrality Parameter - an Overview | {{ScienceDirect Topics}}},
  url = {https://www.sciencedirect.com/topics/mathematics/noncentrality-parameter},
  urldate = {2019-06-13},
  file = {C\:\\Users\\User\\Zotero\\storage\\JTWNXEPU\\noncentrality-parameter.html}
}

@book{okashaPhilosophyScienceVery2016,
  title = {Philosophy of {{Science}}: {{Very Short Introduction}}},
  author = {Okasha, Samir},
  date = {2016},
  publisher = {{Oxford University Press}},
  isbn = {0-19-106278-2}
}

@thesis{pawlowskiMachineLearningProblems2019,
  title = {Machine Learning for Problems with Missing and Uncertain Data with Applications to Personalized Medicine},
  author = {Pawlowski, Colin},
  date = {2019},
  institution = {{Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/handle/1721.1/122473},
  urldate = {2020-01-24},
  abstract = {When we try to apply statistical learning in real-world applications, we frequently encounter data which include missing and uncertain values. This thesis explores the problem of learning from missing and uncertain data with a focus on applications in personalized medicine. In the first chapter, we present a framework for classification when data is uncertain that is based upon robust optimization. We show that adding robustness in both the features and labels results in tractable optimization problems for three widely used classification methods: support vector machines, logistic regression, and decision trees. Through experiments on 75 benchmark data sets, we characterize the learning tasks for which adding robustness provides the most value. In the second chapter, we develop a family of methods for missing data imputation based upon predictive methods and formal optimization.},
  file = {C\:\\Users\\User\\Zotero\\storage\\498EBUGT\\Pawlowski - 2019 - Machine learning for problems with missing and unc.pdf;C\:\\Users\\User\\Zotero\\storage\\GW57468C\\122473.html},
  langid = {english},
  type = {Thesis}
}

@article{pawlowskiMachineLearningProblems2019a,
  title = {Machine {{Learning}} for {{Problems}} with {{Missing}} and {{Uncertain Data}} with {{Applications}} to {{Personalized Medicine}}},
  author = {Pawlowski, Colin},
  date = {2019},
  pages = {215},
  abstract = {When we try to apply statistical learning in real-world applications, we frequently encounter data which include missing and uncertain values. This thesis explores the problem of learning from missing and uncertain data with a focus on applications in personalized medicine.},
  file = {C\:\\Users\\User\\Zotero\\storage\\CFSQZ2GF\\Pawlowski - Machine Learning for Problems with Missing and Unc.pdf},
  langid = {english}
}

@online{PDFEquivalenceTests,
  title = {(1) ({{PDF}}) {{Equivalence}} Tests – {{A}} Review},
  journaltitle = {ResearchGate},
  url = {https://www.researchgate.net/publication/257390208_Equivalence_tests_-_A_review},
  urldate = {2019-06-05},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {C\:\\Users\\User\\Zotero\\storage\\4SVB2ENI\\(1) (PDF) Equivalence tests – A review.html},
  langid = {english}
}

@online{PhilosophyPracticeBayesian,
  title = {Philosophy and the Practice of {{Bayesian}} Statistics - {{Gelman}} - 2013 - {{British Journal}} of {{Mathematical}} and {{Statistical Psychology}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/j.2044-8317.2011.02037.x},
  urldate = {2020-01-31},
  file = {C\:\\Users\\User\\Zotero\\storage\\HF9MHS8E\\j.2044-8317.2011.02037.html}
}

@book{pigottAdvancesMetaAnalysis2012,
  title = {Advances in {{Meta}}-{{Analysis}}},
  author = {Pigott, Terri},
  date = {2012-01-31},
  publisher = {{Springer Science \& Business Media}},
  abstract = {The subject of the book is advanced statistical analyses for quantitative research synthesis (meta-analysis), and selected practical issues relating to research synthesis that are not covered in detail in the many existing introductory books on research synthesis (or meta-analysis). Complex statistical issues are arising more frequently as the primary research that is summarized in quantitative syntheses itself becomes more complex, and as researchers who are conducting meta-analyses become more ambitious in the questions they wish to address. Also as researchers have gained more experience in conducting research syntheses, several key issues have persisted and now appear fundamental to the enterprise of summarizing research.Specifically the book describes multivariate analyses for several indices commonly used in meta-analysis (e.g., correlations, effect sizes, proportions and/or odds ratios), will outline how to do power analysis for meta-analysis (again for each of the different kinds of study outcome indices), and examines issues around research quality and research design and their roles in synthesis. For each of the statistical topics we will examine the different possible statistical models (i.e., fixed, random, and mixed models) that could be adopted by a researcher. In dealing with the issues of study quality and research design it covers a number of specific topics that are of broad concern to research synthesists. In many fields a current issue is how to make sense of results when studies using several different designs appear in a research literature (e.g., Morris \& Deshon, 1997, 2002). In education and other social sciences a critical aspect of this issue is how one might incorporate qualitative (e.g., case study) research within a synthesis. In medicine, related issues concern whether and how to summarize observational studies, and whether they should be combined with randomized controlled trials (or even if they should be combined at all). For each topic, included is a worked example (e.g., for the statistical analyses) and/or a detailed description of a published research synthesis that deals with the practical (non-statistical) issues covered.},
  eprint = {4xhYLmZq5qMC},
  eprinttype = {googlebooks},
  isbn = {978-1-4614-2277-8},
  keywords = {Mathematics / Probability & Statistics / General,Social Science / Research,Social Science / Statistics},
  langid = {english},
  pagetotal = {166}
}

@online{PREPRINTLakensEtal,
  title = {{{PREPRINT}}\_{{Lakens}}\_etal\_{{EquivalenceTestingTutorial}}\_20171117.Pdf},
  journaltitle = {Google Docs},
  url = {https://drive.google.com/a/students.uu.nl/file/d/19vPmFODghUYR145qj4EXOAlD6iY_mJNM/view?usp=drive_open&usp=embed_facebook},
  urldate = {2019-06-05},
  file = {C\:\\Users\\User\\Zotero\\storage\\DZJ3XJCT\\PREPRINT_Lakens_etal_EquivalenceTestingTutorial_20.pdf;C\:\\Users\\User\\Zotero\\storage\\6T28RXLL\\view.html}
}

@online{QuantifyingHeterogeneityMeta,
  title = {Quantifying Heterogeneity in a Meta‐analysis - {{Higgins}} - 2002 - {{Statistics}} in {{Medicine}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.1186},
  urldate = {2019-06-13}
}

@software{queirozTidytextTextMining2020,
  title = {Tidytext: {{Text Mining}} Using 'Dplyr', 'Ggplot2', and {{Other Tidy Tools}}},
  shorttitle = {Tidytext},
  author = {Queiroz, Gabriela De and Fay, Colin and Hvitfeldt, Emil and Keyes, Os and Misra, Kanishka and Mastny, Tim and Erickson, Jeff and Robinson, David and Silge  [aut, Julia and {cre}},
  date = {2020-03-04},
  url = {https://CRAN.R-project.org/package=tidytext},
  urldate = {2020-04-07},
  abstract = {Text mining for word processing and sentiment analysis using 'dplyr', 'ggplot2', and other tidy tools.},
  keywords = {NaturalLanguageProcessing},
  version = {0.2.3}
}

@book{R,
  title = {: {{A}} Language and Environment for Statistical Computing},
  author = {{Core Team}},
  date = {2017},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{Foundation for Statistical Computing}}
}

@book{R,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2020},
  location = {{Vienna, Austria}}
}

@report{raft91,
  title = {How Many Iterations in the {{Gibbs}} Sampler?},
  author = {Raftery, Adrian E and Lewis, Steven},
  date = {1991},
  institution = {{Washington University Seatle, Department of Statistics, United States}}
}

@article{ragh01,
  title = {A Multivariate Technique for Multiply Imputing Missing Values Using a Sequence of Regression Models},
  author = {Raghunathan, Trivellore E and Lepkowski, James M and Van Hoewyk, John and Solenberger, Peter},
  date = {2001},
  journaltitle = {Survey methodology},
  shortjournal = {Survey methodology},
  volume = {27},
  pages = {85--96},
  issn = {0714-0045},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZF27VSE7\\Raghunathan e.a. - 2001 - A multivariate technique for multiply imputing mis.pdf},
  number = {1}
}

@report{ragh07,
  title = {Diagnostics for {{Multiple Imputations}}},
  author = {Raghunathan, Trivellore and Bondarenko, Irina},
  date = {2007-11-21},
  institution = {{Social Science Research Network}},
  location = {{Rochester, NY}},
  url = {https://papers.ssrn.com/abstract=1031750},
  urldate = {2019-10-17},
  abstract = {Multiple imputation technique is becoming a popular method for analyzing data with missing values. Several methods have been proposed for creating multiple imputations and most of these methods assume that the data are missing at random (MAR). However, limited diagnostic tools are available to check whether the imputations created by these methods are reasonable. This article develops a set of diagnostic tools based on certain conditional distributions of the observed and imputed values. These conditional distributions should be similar if the assumed model for creating multiple imputations is a good fit. The tools are formulated in terms of numerical summaries and graphical displays and could be easily implemented using the standard complete data software packages. For implementing these methods the exact nature of the model used by the imputer is not needed. The method is illustrated using a data set with large number of variables of different types with varying amount of missing values.},
  file = {C\:\\Users\\User\\Zotero\\storage\\WL7ZL76E\\Raghunathan en Bondarenko - 2007 - Diagnostics for Multiple Imputations.pdf},
  keywords = {Congeniality,Diagnostics,Missing at Random,Propensity score matching},
  langid = {english},
  number = {ID 1031750},
  type = {SSRN Scholarly Paper}
}

@article{railtonDeductiveNomologicalModelProbabilistic1978,
  title = {A {{Deductive}}-{{Nomological Model}} of {{Probabilistic Explanation}}},
  author = {Railton, Peter},
  date = {1978},
  journaltitle = {Philosophy of Science},
  volume = {45},
  pages = {206--226},
  issn = {0031-8248},
  abstract = {It has been the dominant view that probabilistic explanations of particular facts must be inductive in character. I argue here that this view is mistaken, and that the aim of probabilistic explanation is not to demonstrate that the explanandum fact was nomically expectable, but to give an account of the chance mechanism(s) responsible for it. To this end, a deductive-nomological model of probabilistic explanation is developed and defended. Such a model has application only when the probabilities occurring in covering laws can be interpreted as measures of objective chance, expressing the strength of physical propensities. Unlike inductive models of probabilistic explanation, this deductive model stands in no need of troublesome requirements of maximal specificity or epistemic relativization.},
  eprint = {186817},
  eprinttype = {jstor},
  file = {C\:\\Users\\User\\Zotero\\storage\\BL6L3LMN\\Railton - 1978 - A Deductive-Nomological Model of Probabilistic Exp.pdf},
  number = {2}
}

@article{railtonDeductiveNomologicalModelProbabilistic1978a,
  title = {A {{Deductive}}-{{Nomological Model}} of {{Probabilistic Explanation}}},
  author = {Railton, Peter},
  date = {1978},
  journaltitle = {Philosophy of Science},
  volume = {45},
  pages = {206--226},
  issn = {0031-8248},
  abstract = {It has been the dominant view that probabilistic explanations of particular facts must be inductive in character. I argue here that this view is mistaken, and that the aim of probabilistic explanation is not to demonstrate that the explanandum fact was nomically expectable, but to give an account of the chance mechanism(s) responsible for it. To this end, a deductive-nomological model of probabilistic explanation is developed and defended. Such a model has application only when the probabilities occurring in covering laws can be interpreted as measures of objective chance, expressing the strength of physical propensities. Unlike inductive models of probabilistic explanation, this deductive model stands in no need of troublesome requirements of maximal specificity or epistemic relativization.},
  eprint = {186817},
  eprinttype = {jstor},
  number = {2}
}

@book{rcoreteamLanguageEnvironmentStatistical2020,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  date = {2020},
  location = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@book{rosenbergPhilosophySocialScience2015,
  title = {Philosophy of {{Social Science}}},
  author = {Rosenberg, Alexander and Curtain, Tyler},
  date = {2015},
  publisher = {{Westview Press}},
  location = {{Boulder, CO, UNITED STATES}},
  url = {http://ebookcentral.proquest.com/lib/uunl/detail.action?docID=2039728},
  urldate = {2019-11-22},
  abstract = {Philosophy of Social Science provides a tightly argued yet accessible introduction to the philosophical foundations of the human sciences, including economics, anthropology, sociology, political science, psychology, history, and the disciplines emerging at the intersections of these subjects with biology. Philosophy is unavoidable for social scientists because the choices they make in answering questions in their disciplines force them to take sides on philosophical matters. Conversely, the philosophy of social science is equally necessary for philosophers since the social and behavior sciences must inform their understanding of human action, norms, and social institutions. The fifth edition retains from previous editions an illuminating interpretation of the enduring relations between the social sciences and philosophy, and reflects on developments in social research over the past two decades that have informed and renewed debate in the philosophy of social science. An expanded discussion of philosophical anthropology and modern and postmodern critical theory is new for this edition.},
  file = {C\:\\Users\\User\\Zotero\\storage\\2NUQJPBA\\detail.html},
  isbn = {978-0-8133-4990-9},
  keywords = {Philosophy and social sciences.,Social sciences -- Philosophy.,Social sciences.}
}

@book{rosenbergPhilosophySocialScience2015a,
  title = {Philosophy of {{Social Science}}},
  author = {Rosenberg, Alexander and Curtain, Tyler},
  date = {2015},
  publisher = {{Westview Press}},
  location = {{Boulder, CO, UNITED STATES}},
  url = {http://ebookcentral.proquest.com/lib/uunl/detail.action?docID=2039728},
  urldate = {2020-01-19},
  abstract = {Philosophy of Social Science provides a tightly argued yet accessible introduction to the philosophical foundations of the human sciences, including economics, anthropology, sociology, political science, psychology, history, and the disciplines emerging at the intersections of these subjects with biology. Philosophy is unavoidable for social scientists because the choices they make in answering questions in their disciplines force them to take sides on philosophical matters. Conversely, the philosophy of social science is equally necessary for philosophers since the social and behavior sciences must inform their understanding of human action, norms, and social institutions. The fifth edition retains from previous editions an illuminating interpretation of the enduring relations between the social sciences and philosophy, and reflects on developments in social research over the past two decades that have informed and renewed debate in the philosophy of social science. An expanded discussion of philosophical anthropology and modern and postmodern critical theory is new for this edition.},
  file = {C\:\\Users\\User\\Zotero\\storage\\JHX9ACWW\\detail.html},
  isbn = {978-0-8133-4990-9},
  keywords = {Philosophy and social sciences.,Social sciences -- Philosophy.,Social sciences.}
}

@article{rosenkrantzInductivismProbabilism1971,
  title = {Inductivism and Probabilism},
  author = {Rosenkrantz, Roger},
  date = {1971-12-01},
  journaltitle = {Synthese},
  shortjournal = {Synthese},
  volume = {23},
  pages = {167--205},
  issn = {1573-0964},
  doi = {10.1007/BF00413626},
  abstract = {I I set out my view that all inference is essentially deductive and pinpoint what I take to be the major shortcomings of the induction rule. II The import of data depends on the probability model of the experiment, a dependence ignored by the induction rule. Inductivists admit background knowledge must be taken into account but never spell out how this is to be done. As I see it, that is the problem of induction. III The induction rule, far from providing a method of discovery, does not even serve to detect pattern. Knowing that there is uniformity in the universe is no help to discovering laws. A critique of Reichenbach's justification of the straight rule is constructed along these lines. IV The induction rule, by itself, cannot account for the varying rates at which confidence in an hypothesis mounts with data. The mathematical analysis of this salient feature of inductive reasoning requires prior probabilities. We also argue, against orthodox statisticians, that prior probabilities make a substantive contribution to the objectivity of inductive methods, viz. to the design of experiments and the selection of decision rules. V Carnap's general criticisms of various estimation rules, like the straight rule and the ‘impervious rule’, are seen to be misguided when the prior densities to which they correspond are taken into account. VI Analysis of Hempel's definition of confirmation qua formalization of the enumerative (naive) conception of instancehood. We show that from the standpoint of the quantitative measure P(H/E):P(H) for the degree to which E confirms H, Hempel's classificatory concept yields correct results only for sampling at large from a finite population with a two-way classification all of whose compositions are equally probable. We extend the analysis to Goodman's paradox, finding cases in which grue-like hypotheses do receive as much confirmation as their opposite numbers. We argue, moreover, the irrelevancy of entrenchment, and maintain that Goodman's paradox is no more than a straightforward counter-example to the enumerative conception of instancehood embodied in Hempel's definition. VII We rebutt the objection that prior probabilities, qua inputs of Bayesian analysis, can only be obtained by enumerative induction (insofar as they are objective). The divergence in the prior densities of two rational agents is less a function of subjectivity, we maintain, than of vagueness. VIII Our concluding remarks stress that, for Bayesians, there is no problem of induction in the usual sense.},
  langid = {english},
  number = {2}
}

@article{rosenkrantzInductivismProbabilism1971a,
  title = {Inductivism and Probabilism},
  author = {Rosenkrantz, Roger},
  date = {1971-12-01},
  journaltitle = {Synthese},
  shortjournal = {Synthese},
  volume = {23},
  pages = {167--205},
  issn = {1573-0964},
  doi = {10.1007/BF00413626},
  abstract = {I I set out my view that all inference is essentially deductive and pinpoint what I take to be the major shortcomings of the induction rule. II The import of data depends on the probability model of the experiment, a dependence ignored by the induction rule. Inductivists admit background knowledge must be taken into account but never spell out how this is to be done. As I see it, that is the problem of induction. III The induction rule, far from providing a method of discovery, does not even serve to detect pattern. Knowing that there is uniformity in the universe is no help to discovering laws. A critique of Reichenbach's justification of the straight rule is constructed along these lines. IV The induction rule, by itself, cannot account for the varying rates at which confidence in an hypothesis mounts with data. The mathematical analysis of this salient feature of inductive reasoning requires prior probabilities. We also argue, against orthodox statisticians, that prior probabilities make a substantive contribution to the objectivity of inductive methods, viz. to the design of experiments and the selection of decision rules. V Carnap's general criticisms of various estimation rules, like the straight rule and the ‘impervious rule’, are seen to be misguided when the prior densities to which they correspond are taken into account. VI Analysis of Hempel's definition of confirmation qua formalization of the enumerative (naive) conception of instancehood. We show that from the standpoint of the quantitative measure P(H/E):P(H) for the degree to which E confirms H, Hempel's classificatory concept yields correct results only for sampling at large from a finite population with a two-way classification all of whose compositions are equally probable. We extend the analysis to Goodman's paradox, finding cases in which grue-like hypotheses do receive as much confirmation as their opposite numbers. We argue, moreover, the irrelevancy of entrenchment, and maintain that Goodman's paradox is no more than a straightforward counter-example to the enumerative conception of instancehood embodied in Hempel's definition. VII We rebutt the objection that prior probabilities, qua inputs of Bayesian analysis, can only be obtained by enumerative induction (insofar as they are objective). The divergence in the prior densities of two rational agents is less a function of subjectivity, we maintain, than of vagueness. VIII Our concluding remarks stress that, for Bayesians, there is no problem of induction in the usual sense.},
  langid = {english},
  number = {2}
}

@article{rubin04,
  title = {The {{Design}} of a {{General}} and {{Flexible System}} for {{Handling Nonresponse}} in {{Sample Surveys}}},
  author = {Rubin, Donald B},
  date = {2004-11-01},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {58},
  pages = {298--302},
  doi = {10.1198/000313004X6355},
  file = {C\:\\Users\\User\\Zotero\\storage\\RJ2HAQHD\\Rubin - 2004 - The Design of a General and Flexible System for Ha.pdf;C\:\\Users\\User\\Zotero\\storage\\EN6WGCA4\\000313004X6355.html},
  number = {4}
}

@online{Rubin1972Missing,
  title = {Rubin 1972 Missing - {{Google}} Zoeken},
  url = {https://www.google.com/search?ei=vFqHXrPlCpWDi-gP0difqAM&q=rubin+1972+missing&oq=rubin+1972+missing&gs_lcp=CgZwc3ktYWIQA1CnVFifXmDgYWgAcAB4AIABlgGIAdMDkgEDMS4zmAEAoAEBqgEHZ3dzLXdpeg&sclient=psy-ab&ved=0ahUKEwizs4fCzczoAhWVwQIHHVHsBzUQ4dUDCAs&uact=5},
  urldate = {2020-04-03},
  file = {C\:\\Users\\User\\Zotero\\storage\\5Q2LMQ9T\\search.html}
}

@article{rubin76,
  title = {Inference and {{Missing Data}}},
  author = {Rubin, Donald B.},
  date = {1976},
  journaltitle = {Biometrika},
  volume = {63},
  pages = {581--592},
  doi = {10.2307/2335739},
  abstract = {When making sampling distribution inferences about the parameter of the data, θ, it is appropriate to ignore the process that causes missing data if the missing data are `missing at random' and the observed data are `observed at random', but these inferences are generally conditional on the observed pattern of missing data. When making direct-likelihood or Bayesian inferences about θ, it is appropriate to ignore the process that causes missing data if the missing data are missing at random and the parameter of the missing data process is `distinct' from θ. These conditions are the weakest general conditions under which ignoring the process that causes missing data always leads to correct inferences.},
  number = {3}
}

@book{rubin87,
  title = {Multiple {{Imputation}} for Nonresponse in Surveys},
  author = {Rubin, Donald B.},
  date = {1987},
  publisher = {{Wiley}},
  location = {{New York, NY}},
  file = {C\:\\Users\\User\\Zotero\\storage\\63UKR3PY\\Rubin - 1987 - Multiple Imputation for nonresponse in surveys.pdf},
  langid = {english},
  pagetotal = {258},
  series = {Wiley Series in Probability and Mathematical Statistics {{Applied}} Probability and Statistics}
}

@article{rubin96,
  title = {Multiple {{Imputation After}} 18+ {{Years}}},
  author = {Rubin, Donald B.},
  date = {1996},
  journaltitle = {Journal of the American Statistical Association},
  volume = {91},
  pages = {473--489},
  doi = {10.2307/2291635},
  abstract = {[Multiple imputation was designed to handle the problem of missing data in public-use data bases where the data-base constructor and the ultimate user are distinct entities. The objective is valid frequency inference for ultimate users who in general have access only to complete-data software and possess limited knowledge of specific reasons and models for nonresponse. For this situation and objective, I believe that multiple imputation by the data-base constructor is the method of choice. This article first provides a description of the assumed context and objectives, and second, reviews the multiple imputation framework and its standard results. These preliminary discussions are especially important because some recent commentaries on multiple imputation have reflected either misunderstandings of the practical objectives of multiple imputation or misunderstandings of fundamental theoretical results. Then, criticisms of multiple imputation are considered, and, finally, comparisons are made to alternative strategies.]},
  file = {C\:\\Users\\User\\Zotero\\storage\\9UVT28MF\\Rubin - Multiple Imputation after 18+ years.pdf},
  number = {434}
}

@article{ruiterAppealMethodologicalFusion2017,
  title = {An {{Appeal}} for a {{Methodological Fusion}} of {{Conversation Analysis}} and {{Experimental Psychology}}},
  author = {de Ruiter, J. P. and Albert, Saul},
  date = {2017-01-02},
  journaltitle = {Research on Language and Social Interaction},
  volume = {50},
  pages = {90--107},
  issn = {0835-1813},
  doi = {10.1080/08351813.2017.1262050},
  abstract = {Human social interaction is studied by researchers in conversation analysis (CA) and psychology, but the dominant methodologies within these two disciplines are very different. Analyzing methodological differences in relation to major developments in the philosophy of science, we suggest that a central difference is that psychologists tend to follow Popper’s falsificationism in dissociating the context of discovery and the context of justification. In CA, following Garfinkel’s ethnomethodology, these two contexts are much closer to one another, if not inextricable. While this dissociation allows the psychologist a much larger theoretical freedom, because psychologists “only” need to validate their theories by generating confirmed predictions from experiments, it also carries the risk of generating theories that are less robust and pertinent to everyday interaction than the body of knowledge accumulated by CA. However, as long as key philosophical differences are well understood, it is not an inherently bad idea to generate predictions from theories and use quantitative and experimental methods to test them. It is both desirable and achievable to find a synthesis between methodologies that combines their strengths and avoids their weaknesses. We discuss a number of challenges that would need to be met and some opportunities that may arise from creating such a synthesis.},
  file = {C\:\\Users\\User\\Zotero\\storage\\T6TD6TD5\\Ruiter en Albert - 2017 - An Appeal for a Methodological Fusion of Conversat.pdf;C\:\\Users\\User\\Zotero\\storage\\QIEE6UUV\\08351813.2017.html},
  number = {1}
}

@online{SampleSizeDeterminations,
  title = {Sample Size Determinations for {{Welch}}'s Test in One‐way Heteroscedastic {{ANOVA}} - {{Jan}} - 2014 - {{British Journal}} of {{Mathematical}} and {{Statistical Psychology}} - {{Wiley Online Library}}},
  url = {https://onlinelibrary.wiley.com/doi/full/10.1111/bmsp.12006?sid=nlm%3Apubmed},
  urldate = {2019-06-19},
  file = {C\:\\Users\\User\\Zotero\\storage\\WFME75Q6\\bmsp.html}
}

@article{sas14,
  title = {Sensitivity {{Analysis}} in {{Multiple Imputation}} for {{Missing Data}}},
  author = {Yuan, Yang},
  journaltitle = {In Proceedings of the SAS Global Forum 2014 Conference},
  abstract = {Multiple imputation, a popular strategy for dealing with missing values, usually assumes that the data are missing at random (MAR). That is, for a variable Y, the probability that an observation is missing depends only on the observed values of other variables, not on the unobserved values of Y. It is important to examine the sensitivity of inferences to departures from the MAR assumption, because this assumption cannot be verified using the data.},
  file = {C\:\\Users\\User\\Zotero\\storage\\NQKZC84X\\Yuan - Sensitivity Analysis in Multiple Imputation for Mi.pdf},
  langid = {english}
}

@article{scha02,
  title = {Missing Data: Our View of the State of the Art.},
  author = {Schafer, Joseph L and Graham, John W},
  date = {2002},
  journaltitle = {Psychological methods},
  shortjournal = {Psychological methods},
  volume = {7},
  pages = {147},
  issn = {1939-1463},
  file = {C\:\\Users\\User\\Zotero\\storage\\RP4NJ9T2\\Schafer en Graham - 2002 - Missing data our view of the state of the art..pdf},
  number = {2}
}

@book{scha97,
  title = {Analysis of Incomplete Multivariate Data},
  author = {Schafer, Joseph L},
  date = {1997},
  publisher = {{Chapman and Hall/CRC}},
  file = {C\:\\Users\\User\\Zotero\\storage\\Q7G6U2VV\\Schafer - 1997 - Analysis of incomplete multivariate data.pdf;C\:\\Users\\User\\Zotero\\storage\\VEI29NIR\\Schafer - 1997 - Analysis of incomplete multivariate data.pdf}
}

@article{schaferMultipleImputationMultivariate1998,
  title = {Multiple {{Imputation}} for {{Multivariate Missing}}-{{Data Problems}}: {{A Data Analyst}}'s {{Perspective}}},
  shorttitle = {Multiple {{Imputation}} for {{Multivariate Missing}}-{{Data Problems}}},
  author = {Schafer, Joseph L. and Olsen, Maren K.},
  date = {1998-10-01},
  journaltitle = {Multivariate Behavioral Research},
  volume = {33},
  pages = {545--571},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr3304_5},
  abstract = {Analyses of multivariate data are frequently hampered by missing values. Until recently, the only missing-data methods available to most data analysts have been relatively ad1 hoc practices such as listwise deletion. Recent dramatic advances in theoretical and computational statistics, however, have produced anew generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simulation technique that replaces each missing datum with a set of m {$>$} 1 plausible values. The rn versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997a) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from the Adolescent Alcohol Prevention Trial (Hansen \& Graham, 199 I).},
  eprint = {26753828},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\4GAS94IR\\Multiple Imputation for Multivariate Missing Data Problems A Data Analyst s Perspective.pdf;C\:\\Users\\User\\Zotero\\storage\\LAV5Z2LK\\Schafer en Olsen - 1998 - Multiple Imputation for Multivariate Missing-Data .pdf;C\:\\Users\\User\\Zotero\\storage\\G6SW2LHQ\\s15327906mbr3304_5.html},
  number = {4}
}

@article{schaferMultipleImputationMultivariate1998a,
  title = {Multiple {{Imputation}} for {{Multivariate Missing}}-{{Data Problems}}: {{A Data Analyst}}'s {{Perspective}}},
  author = {Schafer, Joseph L. and Olsen, Maren K.},
  date = {1998-10-01},
  journaltitle = {Multivariate Behavioral Research},
  shortjournal = {Multivariate Behavioral Research},
  volume = {33},
  pages = {545--571},
  publisher = {{Routledge}},
  issn = {0027-3171},
  doi = {10.1207/s15327906mbr3304_5},
  number = {4}
}

@unpublished{schw09,
  title = {Draft {{Users Manual}}: {{Proper Use}} of the {{Schwarz Value Survey}}, Version 14 {{January}} 2009, Compiled by {{Romie F}}. {{Littrell}}. {{Auckland}}, {{New Zealand}}: {{Centre}} for {{Cross Cultural Comparisons}}},
  author = {Schwartz, Shalom H},
  date = {2009}
}

@article{schw12,
  title = {An {{Overview}} of the {{Schwartz Theory}} of {{Basic Values}}},
  author = {Schwartz, Shalom H.},
  date = {2012-12-01},
  journaltitle = {Online Readings in Psychology and Culture},
  shortjournal = {Online Readings in Psychology and Culture},
  volume = {2},
  issn = {2307-0919},
  doi = {10.9707/2307-0919.1116},
  abstract = {This article presents an overview of the Schwartz theory of basic human values. It discusses the nature of values and spells out the features that are common to all values and what distinguishes one value from another. The theory identifies ten basic personal values that are recognized across cultures and explains where they come from. At the heart of the theory is the idea that values form a circular structure that reflects the motivations each value expresses. This circular structure, that captures the conflicts and compatibility among the ten values is apparently culturally universal. The article elucidates the psychological principles that give rise to it. Next, it presents the two major methods developed to measure the basic values, the Schwartz Value Survey and the Portrait Values Questionnaire. Findings from 82 countries, based on these and other methods, provide evidence for the validity of the theory across cultures. The findings reveal substantial differences in the value priorities of individuals. Surprisingly, however, the average value priorities of most societal groups exhibit a similar hierarchical order whose existence the article explains. The last section of the article clarifies how values differ from other concepts used to explain behavior—attitudes, beliefs, norms, and traits.},
  file = {C\:\\Users\\User\\Zotero\\storage\\KFS4VH2T\\Schwartz - 2012 - An Overview of the Schwartz Theory of Basic Values.pdf},
  langid = {english},
  number = {1}
}

@article{schwartzDraftUserManual,
  title = {Draft User’s Manual. {{Proper}} Use of the {{Schwartz Value Survey}}},
  author = {Schwartz, Shalom H. and Littrell, Romie F.},
  journaltitle = {Unpublished manuscript, Auckland, New Zealand},
  url = {https://www.academia.edu/29396758/Draft_user_s_manual._Proper_use_of_the_Schwartz_Value_Survey},
  urldate = {2020-04-28},
  abstract = {Draft user’s manual. Proper use of the Schwartz Value Survey},
  file = {C\:\\Users\\User\\Zotero\\storage\\H3VTECHE\\Draft_user_s_manual.html},
  langid = {english}
}

@article{schwartzPersonalityGenderAge2013,
  title = {Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach},
  shorttitle = {Personality, Gender, and Age in the Language of Social Media},
  author = {Schwartz, H. Andrew and Eichstaedt, Johannes C. and Kern, Margaret L. and Dziurzynski, Lukasz and Ramones, Stephanie M. and Agrawal, Megha and Shah, Achal and Kosinski, Michal and Stillwell, David and Seligman, Martin E. P. and Ungar, Lyle H.},
  date = {2013},
  journaltitle = {PloS One},
  shortjournal = {PLoS ONE},
  volume = {8},
  pages = {e73791},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  abstract = {We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our open-vocabulary technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase 'sick of' and the word 'depressed'), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive 'my' when mentioning their 'wife' or 'girlfriend' more often than females use 'my' with 'husband' or 'boyfriend'). To date, this represents the largest study, by an order of magnitude, of language and personality.},
  eprint = {24086296},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\RJ7RPFXA\\Schwartz e.a. - 2013 - Personality, gender, and age in the language of so.pdf},
  keywords = {Age Factors,Female,Humans,Language,Male,Personality,Sex Factors,Social Media,Vocabulary},
  langid = {english},
  number = {9},
  pmcid = {PMC3783449}
}

@article{schwartzPersonalityGenderAge2013a,
  title = {Personality, {{Gender}}, and {{Age}} in the {{Language}} of {{Social Media}}: {{The Open}}-{{Vocabulary Approach}}},
  shorttitle = {Personality, {{Gender}}, and {{Age}} in the {{Language}} of {{Social Media}}},
  author = {Schwartz, H. Andrew and Eichstaedt, Johannes C. and Kern, Margaret L. and Dziurzynski, Lukasz and Ramones, Stephanie M. and Agrawal, Megha and Shah, Achal and Kosinski, Michal and Stillwell, David and Seligman, Martin E. P. and Ungar, Lyle H.},
  date = {2013-09-25},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  pages = {e73791},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0073791},
  abstract = {We analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age. In our open-vocabulary technique, the data itself drives a comprehensive exploration of language that distinguishes people, finding connections that are not captured with traditional closed-vocabulary word-category analyses. Our analyses shed new light on psychosocial processes yielding results that are face valid (e.g., subjects living in high elevations talk about the mountains), tie in with other research (e.g., neurotic people disproportionately use the phrase ‘sick of’ and the word ‘depressed’), suggest new hypotheses (e.g., an active life implies emotional stability), and give detailed insights (males use the possessive ‘my’ when mentioning their ‘wife’ or ‘girlfriend’ more often than females use ‘my’ with ‘husband’ or 'boyfriend’). To date, this represents the largest study, by an order of magnitude, of language and personality.},
  file = {C\:\\Users\\User\\Zotero\\storage\\K7NES4SF\\Schwartz e.a. - 2013 - Personality, Gender, and Age in the Language of So.pdf;C\:\\Users\\User\\Zotero\\storage\\7FE4ZZ3R\\article.html},
  keywords = {Emotions,Facebook,Forecasting,Language,Personality,Psycholinguistics,Semantics,Social media},
  langid = {english},
  number = {9}
}

@article{SemanticValidity2020,
  title = {Semantic {{Validity}}},
  date = {2020-06-19},
  publisher = {{OSF}},
  doi = {None},
  abstract = {Hosted on the Open Science Framework},
  file = {C\:\\Users\\User\\Zotero\\storage\\RZEAKFQC\\awhrq.html},
  langid = {english}
}

@article{shahArtificialIntelligenceMachine2019,
  title = {Artificial Intelligence and Machine Learning in Clinical Development: A Translational Perspective},
  shorttitle = {Artificial Intelligence and Machine Learning in Clinical Development},
  author = {Shah, Pratik and Kendall, Francis and Khozin, Sean and Goosen, Ryan and Hu, Jianying and Laramie, Jason and Ringel, Michael and Schork, Nicholas},
  date = {2019-07-26},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digit. Med.},
  volume = {2},
  pages = {1--5},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0148-3},
  abstract = {Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, computing power to identify clinically meaningful patterns in the data using efficient artificial intelligence and machine-learning algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights, recent developments, and recommendations for infusing actionable computational evidence into clinical development and health care from academy, biotechnology industry, nonprofit foundations, regulators, and technology corporations. Analysis and learning from publically available biomedical and clinical trial data sets, real-world evidence from sensors, and health records by machine-learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI- and ML-based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing applications and impact of digital algorithmic evidence to improve medical care for patients.},
  file = {C\:\\Users\\User\\Zotero\\storage\\ES8XJPJ2\\Shah e.a. - 2019 - Artificial intelligence and machine learning in cl.pdf;C\:\\Users\\User\\Zotero\\storage\\T8LPPJTV\\s41746-019-0148-3.html},
  langid = {english},
  number = {1}
}

@article{shahArtificialIntelligenceMachine2019a,
  title = {Artificial Intelligence and Machine Learning in Clinical Development: A Translational Perspective},
  author = {Shah, Pratik and Kendall, Francis and Khozin, Sean and Goosen, Ryan and Hu, Jianying and Laramie, Jason and Ringel, Michael and Schork, Nicholas},
  date = {2019-07-26},
  journaltitle = {npj Digital Medicine},
  shortjournal = {npj Digital Medicine},
  volume = {2},
  pages = {69},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0148-3},
  abstract = {Future of clinical development is on the verge of a major transformation due to convergence of large new digital data sources, computing power to identify clinically meaningful patterns in the data using efficient artificial intelligence and machine-learning algorithms, and regulators embracing this change through new collaborations. This perspective summarizes insights, recent developments, and recommendations for infusing actionable computational evidence into clinical development and health care from academy, biotechnology industry, nonprofit foundations, regulators, and technology corporations. Analysis and learning from publically available biomedical and clinical trial data sets, real-world evidence from sensors, and health records by machine-learning architectures are discussed. Strategies for modernizing the clinical development process by integration of AI- and ML-based digital methods and secure computing technologies through recently announced regulatory pathways at the United States Food and Drug Administration are outlined. We conclude by discussing applications and impact of digital algorithmic evidence to improve medical care for patients.},
  number = {1}
}

@article{shiny,
  title = {Shiny: Web Application Framework for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, JJ and Xie, Yihui and McPherson, Jonathan},
  date = {2017},
  url = {https://CRAN.R-project.org/package=shiny}
}

@software{shiny17,
  title = {Shiny: {{Web Application Framework}} for {{R}}},
  shorttitle = {Shiny},
  author = {Chang, Winston and Cheng, Joe and Allaire, J. J. and Xie, Yihui and McPherson, Jonathan and RStudio and {library)}, jQuery Foundation (jQuery library {and} jQuery UI and library; authors listed in inst/www/shared/jquery- AUTHORS.txt), jQuery contributors (jQuery and library; authors listed in {inst/www/shared/jqueryui/AUTHORS.txt)}, jQuery UI contributors (jQuery UI and {library)}, Mark Otto (Bootstrap and {library)}, Jacob Thornton (Bootstrap and {library)}, Bootstrap contributors (Bootstrap and Twitter and {library)}, Inc (Bootstrap and {library)}, Alexander Farkas (html5shiv and js {library)}, Scott Jehl (Respond and {library)}, Stefan Petre (Bootstrap-datepicker and {library)}, Andrew Rowls (Bootstrap-datepicker and {font)}, Dave Gandy (Font-Awesome and js {library)}, Brian Reavis (selectize and {library)}, Kristopher Michael Kowal (es5-shim and {library)}, es5-shim contributors (es5-shim and rangeSlider {library)}, Denis Ineshin (ion and strftime {library)}, Sami Samhuri (Javascript and {library)}, SpryMedia Limited (DataTables and js {library)}, John Fraser (showdown and js {library)}, John Gruber (showdown and js {library)}, Ivan Sagalaev (highlight and implementation from R), R. Core Team (tar},
  date = {2019-04-22},
  url = {https://CRAN.R-project.org/package=shiny},
  urldate = {2019-09-05},
  abstract = {Makes it incredibly easy to build interactive web applications with R. Automatic "reactive" binding between inputs and outputs and extensive prebuilt widgets make it possible to build beautiful, responsive, and powerful applications with minimal effort.},
  annotation = {Add package version!},
  keywords = {TeachingStatistics,WebTechnologies},
  options = {useprefix=true},
  version = {1.3.2}
}

@book{silgeTextMiningTidy2017,
  title = {Text Mining with {{R}}: A Tidy Approach},
  shorttitle = {Text Mining with {{R}}},
  author = {Silge, Julia and Robinson, David},
  date = {2017},
  url = {tidytextmining.com},
  urldate = {2020-04-07},
  abstract = {Chapter 7. Case Study: Comparing Twitter Archives; Getting the Data and Distribution of Tweets; Word Frequencies; Comparing Word Usage; Changes in Word Use; Favorites and Retweets; Summary; Chapter 8. Case Study: Mining NASA Metadata; How Data Is Organized at NASA; Wrangling and Tidying the Data; Some Initial Simple Exploration; Word Co-ocurrences and Correlations; Networks of Description and Title Words; Networks of Keywords; Calculating tf-idf for the Description Fields; What Is tf-idf for the Description Field Words?; Connecting Description Fields to Keywords; Topic Modeling.},
  annotation = {OCLC: 990182937},
  isbn = {978-1-4919-8162-7 978-1-4919-8160-3 978-1-4919-8165-8},
  langid = {english}
}

@article{Stasinopoulos+Rigby:2007,
  title = {Generalized Additive Models for Location Scale and Shape ({{GAMLSS}}) In},
  author = {Stasinopoulos, D. Mikis and Rigby, Robert A.},
  date = {2007},
  journaltitle = {Journal of Statistical Software},
  volume = {23},
  pages = {1--46},
  doi = {10.18637/jss.v023.i07},
  number = {7}
}

@article{su11,
  title = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}: {{Opening Windows}} into the {{Black Box}}},
  shorttitle = {Multiple {{Imputation}} with {{Diagnostics}} (Mi) in {{R}}},
  author = {Su, Yu-Sung and Gelman, Andrew E. and Hill, Jennifer and Yajima, Masanao},
  date = {2011},
  volume = {45},
  pages = {1--31},
  doi = {10.7916/D8VQ3CD3},
  abstract = {Our mi package in R has several features that allow the user to get inside the imputation process and evaluate the reasonableness of the resulting models and imputations. These features include: choice of predictors, models, and transformations for chained imputation models; standard and binned residual plots for checking the fit of the conditional distributions used for imputation; and plots for comparing the distributions of observed and imputed data. In addition, we use Bayesian models and weakly informative prior distributions to construct more stable estimates of imputation models. Our goal is to have a demonstration package that (a) avoids many of the practical problems that arise with existing multivariate imputation programs, and (b) demonstrates state-of-the-art diagnostics that can be applied more generally and can be incorporated into the software of others.},
  file = {C\:\\Users\\User\\Zotero\\storage\\S5SZMIHW\\Su e.a. - 2011 - Multiple Imputation with Diagnostics (mi) in R Op.pdf;C\:\\Users\\User\\Zotero\\storage\\9KLRX5W9\\D8VQ3CD3.html},
  langid = {english},
  number = {2}
}

@article{taka17,
  title = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}: {{Assessing}} the {{Effects}} of {{Between}}-{{Imputation Iterations}}},
  shorttitle = {Statistical {{Inference}} in {{Missing Data}} by {{MCMC}} and {{Non}}-{{MCMC Multiple Imputation Algorithms}}},
  author = {Takahashi, Masayoshi},
  date = {2017-07-28},
  journaltitle = {Data Science Journal},
  volume = {16},
  pages = {37},
  doi = {10.5334/dsj-2017-037},
  abstract = {Incomplete data are ubiquitous in social sciences; as a consequence, available data are inefficient (ineffective) and often biased. In the literature, multiple imputation is known to be the standard method to handle missing data. While the theory of multiple imputation has been known for decades, the implementation is difficult due to the complicated nature of random draws from the posterior distribution. Thus, there are several computational algorithms in software: Data Augmentation (DA), Fully Conditional Specification (FCS), and Expectation-Maximization with Bootstrapping (EMB). Although the literature is full of comparisons between joint modeling (DA, EMB) and conditional modeling (FCS), little is known about the relative superiority between the MCMC algorithms (DA, FCS) and the non-MCMC algorithm (EMB), where MCMC stands for Markov chain Monte Carlo. Based on simulation experiments, the current study contends that EMB is a confidence proper (confidence-supporting) multiple imputation algorithm without between-imputation iterations; thus, EMB is more user-friendly than DA and FCS.},
  file = {C\:\\Users\\User\\Zotero\\storage\\UC3SL5U6\\Takahashi - 2017 - Statistical Inference in Missing Data by MCMC and .pdf;C\:\\Users\\User\\Zotero\\storage\\UJTMEUVT\\dsj-2017-037.html},
  keywords = {Conditional modeling,Incomplete data,Joint modeling,Markov chain Monte Carlo,MCMC,Nonresponse},
  langid = {english}
}

@misc{tatmanBlogAuthorshipCorpus2017,
  title = {Blog {{Authorship Corpus}}},
  author = {Tatman, Rachael},
  date = {2017},
  url = {https://kaggle.com/rtatman/blog-authorship-corpus},
  urldate = {2020-04-08},
  abstract = {Over 600,000 posts from more than 19 thousand bloggers},
  file = {C\:\\Users\\User\\Zotero\\storage\\XWMV8QDF\\metadata.html},
  langid = {english}
}

@online{TimeSeriesUnderstanding,
  title = {Time Series - {{Understanding}} This Acf Output},
  journaltitle = {Cross Validated},
  url = {https://stats.stackexchange.com/questions/81754/understanding-this-acf-output},
  urldate = {2020-04-17},
  file = {C\:\\Users\\User\\Zotero\\storage\\VQQ2NBF3\\81764.html}
}

@software{usheyReticulateInterfacePython2020,
  title = {Reticulate: {{Interface}} to '{{Python}}'},
  shorttitle = {Reticulate},
  author = {Ushey, Kevin and Allaire, J. J. and RStudio and Tang  [aut, Yuan and {cph} and Eddelbuettel, Dirk and Lewis, Bryan and Keydana, Sigrid and Hafen, Ryan and {library}, Marcus Geelnard (TinyThread and {http://tinythreadpp.bitsnbites.eu/)}},
  date = {2020-04-02},
  url = {https://CRAN.R-project.org/package=reticulate},
  urldate = {2020-04-07},
  abstract = {Interface to 'Python' modules, classes, and functions. When calling into 'Python', R data types are automatically converted to their equivalent 'Python' types. When values are returned from 'Python' to R they are converted back to R types. Compatible with all versions of 'Python' {$>$}= 2.7.},
  keywords = {HighPerformanceComputing,ModelDeployment,NumericalMathematics},
  version = {1.15}
}

@article{vanbuurenMultipleImputationMissing1999,
  title = {Multiple Imputation of Missing Blood Pressure Covariates in Survival Analysis},
  author = {van Buuren, S. and Boshuizen, H. C. and Knook, D. L.},
  date = {1999-03-30},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {18},
  pages = {681--694},
  issn = {0277-6715},
  doi = {10.1002/(sici)1097-0258(19990330)18:6<681::aid-sim71>3.0.co;2-r},
  abstract = {This paper studies a non-response problem in survival analysis where the occurrence of missing data in the risk factor is related to mortality. In a study to determine the influence of blood pressure on survival in the very old (85+ years), blood pressure measurements are missing in about 12.5 per cent of the sample. The available data suggest that the process that created the missing data depends jointly on survival and the unknown blood pressure, thereby distorting the relation of interest. Multiple imputation is used to impute missing blood pressure and then analyse the data under a variety of non-response models. One special modelling problem is treated in detail; the construction of a predictive model for drawing imputations if the number of variables is large. Risk estimates for these data appear robust to even large departures from the simplest non-response model, and are similar to those derived under deletion of the incomplete records.},
  eprint = {10204197},
  eprinttype = {pmid},
  keywords = {Aged,Aged; 80 and over,Blood Pressure,Cohort Studies,Data Collection,Demography,Female,Humans,Male,Meta-Analysis as Topic,Survival Analysis},
  langid = {english},
  number = {6},
  options = {useprefix=true}
}

@article{vanginkelStandardizedRegressionCoefficients2020,
  title = {Standardized {{Regression Coefficients}} and {{Newly Proposed Estimators}} for \$\$\{\vphantom\}{{R}}\vphantom\{\}\^\{\{2\}\}\$\${{R2}} in {{Multiply Imputed Data}}},
  author = {van Ginkel, Joost R.},
  date = {2020-03-11},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  issn = {1860-0980},
  doi = {10.1007/s11336-020-09696-4},
  abstract = {Whenever statistical analyses are applied to multiply imputed datasets, specific formulas are needed to combine the results into one overall analysis, also called combination rules. In the context of regression analysis, combination rules for the unstandardized regression coefficients, the t-tests of the regression coefficients, and the F-tests for testing \$\$R\^\{2\}\$\$R2 for significance have long been established. However, there is still no general agreement on how to combine the point estimators of \$\$R\^\{2\}\$\$R2 in multiple regression applied to multiply imputed datasets. Additionally, no combination rules for standardized regression coefficients and their confidence intervals seem to have been developed at all. In the current article, two sets of combination rules for the standardized regression coefficients and their confidence intervals are proposed, and their statistical properties are discussed. Additionally, two improved point estimators of \$\$R\^\{2\}\$\$R2 in multiply imputed data are proposed, which in their computation use the pooled standardized regression coefficients. Simulations show that the proposed pooled standardized coefficients produce only small bias and that their 95\% confidence intervals produce coverage close to the theoretical 95\%. Furthermore, the simulations show that the newly proposed pooled estimates for \$\$R\^\{2\}\$\$R2 are less biased than two earlier proposed pooled estimates.},
  file = {C\:\\Users\\User\\Zotero\\storage\\Y76EU5BB\\van Ginkel - 2020 - Standardized Regression Coefficients and Newly Pro.pdf},
  langid = {english},
  options = {useprefix=true}
}

@article{vats18,
  title = {Revisiting the {{Gelman}}-{{Rubin Diagnostic}}},
  author = {Vats, Dootika and Knudson, Christina},
  date = {2018-12-21},
  url = {http://arxiv.org/abs/1812.09384},
  urldate = {2019-11-05},
  abstract = {Gelman and Rubin's (1992) convergence diagnostic is one of the most popular methods for terminating a Markov chain Monte Carlo (MCMC) sampler. Since the seminal paper, researchers have developed sophisticated methods of variance estimation for Monte Carlo averages. We show that this class of estimators find immediate use in the Gelman-Rubin statistic, a connection not established in the literature before. We incorporate these estimators to upgrade both the univariate and multivariate Gelman-Rubin statistics, leading to increased stability in MCMC termination time. An immediate advantage is that our new Gelman-Rubin statistic can be calculated for a single chain. In addition, we establish a relationship between the Gelman-Rubin statistic and effective sample size. Leveraging this relationship, we develop a principled cutoff criterion for the Gelman-Rubin statistic. Finally, we demonstrate the utility of our improved diagnostic via examples.},
  archivePrefix = {arXiv},
  eprint = {1812.09384},
  eprinttype = {arxiv},
  file = {C\:\\Users\\User\\Zotero\\storage\\4Z9GIUF9\\Vats en Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;C\:\\Users\\User\\Zotero\\storage\\5WTJRDRB\\Vats en Knudson - 2018 - Revisiting the Gelman-Rubin Diagnostic.pdf;C\:\\Users\\User\\Zotero\\storage\\GLJJJXBA\\1812.html},
  keywords = {Statistics - Computation,Statistics - Methodology},
  primaryClass = {stat}
}

@article{veht19,
  title = {Rank-Normalization, Folding, and Localization: {{An}} Improved \{\$\textbackslash widehat\{\vphantom{\}\}}{{R}}\vphantom\{\}\$\vphantom\{\} for Assessing Convergence of {{MCMC}}},
  shorttitle = {Rank-Normalization, Folding, and Localization},
  author = {Vehtari, Aki and Gelman, Andrew and Simpson, Daniel and Carpenter, Bob and Bürkner, Paul-Christian},
  date = {2019-03-19},
  url = {http://arxiv.org/abs/1903.08008},
  urldate = {2019-09-16},
  abstract = {Markov chain Monte Carlo is a key computational tool in Bayesian statistics, but it can be challenging to monitor the convergence of an iterative stochastic algorithm. In this paper we show that the convergence diagnostic \$\textbackslash widehat\{R\}\$ of Gelman and Rubin (1992) has serious flaws and we propose an alternative that fixes them. We also introduce a collection of quantile-based local efficiency measures, along with a practical approach for computing Monte Carlo error estimates for quantiles. We suggest that common trace plots should be replaced with rank plots from multiple chains. Finally, we give concrete recommendations for how these methods should be used in practice.},
  file = {C\:\\Users\\User\\Zotero\\storage\\UY8ZSDS6\\Vehtari e.a. - 2019 - Rank-normalization, folding, and localization An .pdf;C\:\\Users\\User\\Zotero\\storage\\SZQ7NZQE\\1903.html},
  keywords = {Statistics - Computation,Statistics - Methodology}
}

@book{Venables+Ripley:2002,
  title = {Modern Applied Statistics With},
  author = {Venables, William N. and Ripley, Brian D.},
  date = {2002},
  edition = {4},
  publisher = {{Springer-Verlag}},
  location = {{New York}},
  doi = {10.1007/978-0-387-21706-2},
  pagetotal = {495}
}

@article{vink14,
  title = {Pooling Multiple Imputations When the Sample Happens to Be the Population},
  author = {Vink, Gerko and van Buuren, Stef},
  date = {2014-09-30},
  url = {http://arxiv.org/abs/1409.8542},
  urldate = {2019-12-13},
  abstract = {Current pooling rules for multiply imputed data assume infinite populations. In some situations this assumption is not feasible as every unit in the population has been observed, potentially leading to over-covered population estimates. We simplify the existing pooling rules for situations where the sampling variance is not of interest. We compare these rules to the conventional pooling rules and demonstrate their use in a situation where there is no sampling variance. Using the standard pooling rules in situations where sampling variance should not be considered, leads to overestimation of the variance of the estimates of interest, especially when the amount of missingness is not very large. As a result, populations estimates are over-covered, which may lead to a loss of statistical power. We conclude that the theory of multiple imputation can be extended to the situation where the sample happens to be the population. The simplified pooling rules can be easily implemented to obtain valid inference in cases where we have observed essentially all units and in simulation studies addressing the missingness mechanism only.},
  file = {C\:\\Users\\User\\Zotero\\storage\\9TV8WDM5\\Vink en van Buuren - 2014 - Pooling multiple imputations when the sample happe.pdf;C\:\\Users\\User\\Zotero\\storage\\P6KYWI5H\\1409.html},
  keywords = {Mathematics - Statistics Theory,Statistics - Computation},
  options = {useprefix=true}
}

@article{vinkMultipleImputationSquared2013,
  title = {Multiple {{Imputation}} of {{Squared Terms}}},
  author = {Vink, Gerko and van Buuren, Stef},
  date = {2013-11-01},
  journaltitle = {Sociological Methods \& Research},
  shortjournal = {Sociological Methods \& Research},
  volume = {42},
  pages = {598--607},
  publisher = {{SAGE Publications Inc}},
  issn = {0049-1241},
  doi = {10.1177/0049124113502943},
  abstract = {We propose a new multiple imputation technique for imputing squares. Current methods yield either unbiased regression estimates or preserve data relations. No method, however, seems to deliver both, which limits researchers in the implementation of regression analysis in the presence of missing data. Besides, current methods only work under a missing completely at random (MCAR) mechanism. Our method for imputing squares uses a polynomial combination. The proposed method yields both unbiased regression estimates, while preserving the quadratic relations in the data for both missing at random and MCAR mechanisms.},
  file = {C\:\\Users\\User\\Zotero\\storage\\VBSZWGVA\\Vink en van Buuren - 2013 - Multiple Imputation of Squared Terms.pdf},
  langid = {english},
  number = {4},
  options = {useprefix=true}
}

@article{vinknd,
  title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
  author = {Vink, Gerko},
  year = {\bibstring{nodate}},
  abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
  file = {C\:\\Users\\User\\Zotero\\storage\\ZXGCXPP4\\Vink - Towards a standardized evaluation of multiple impu.pdf},
  langid = {english}
}

@article{vinkStandardizedEvaluationMultiple,
  title = {Towards a Standardized Evaluation of Multiple Imputation Routines},
  author = {Vink, Gerko},
  pages = {16},
  abstract = {Developing new imputation methodology has become a very active field. Unfortunately, there is no consensus on how to perform simulation studies to evaluate the properties of imputation methods. In this paper I propose a move towards a standardized evaluation of imputation methods. To demonstrate the need for standardization, I highlight a set of potential pitfalls that bring forth a chain of potential problems in the objective assessment of the performance of imputation routines. This may lead to suboptimal use of multiple imputation in practice. Additionally, I suggest a course of action for simulating and evaluating missing data problems.},
  file = {C\:\\Users\\User\\Zotero\\storage\\6CDII8NH\\Vink - Towards a standardized evaluation of multiple impu.pdf},
  langid = {english}
}

@book{wangEradicatingIdeologicalViruses2018,
  title = {"{{Eradicating}} Ideological Viruses": {{China}}'s Campaign of Repression against {{Xinjiang}}'s {{Muslims}}},
  shorttitle = {"{{Eradicating}} Ideological Viruses"},
  author = {Wang, Maya},
  date = {2018},
  publisher = {{Human Rights Watch}},
  location = {{New York, N.Y.}},
  abstract = {"This report presents new evidence of the Chinese government's mass arbitrary detention, torture, and mistreatment, and the increasingly pervasive controls on daily life. Throughout the region, the Turkic Muslim population of 13 million is subjected to forced political indoctrination, collective punishment, restrictions on movement and communications, heightened religious restrictions, and mass surveillance in violation of international human rights law."--Publisher website, viewed September 19, 2018},
  annotation = {OCLC: on1052895278},
  editora = {Human Rights Watch (Organization)},
  editoratype = {collaborator},
  file = {C\:\\Users\\User\\Zotero\\storage\\IAZ3Z7ZJ\\Wang - 2018 - Eradicating ideological viruses China's campaig.pdf},
  isbn = {978-1-62313-656-7},
  keywords = {China,China Xinjiang Uygur Zizhiqu,Ethnic relations,Human rights,Muslims,National security,Political persecution,Social conditions,Social policy,Turkic peoples,Xinjiang Uygur Zizhiqu,Xinjiang Uygur Zizhiqu (China)},
  langid = {english},
  pagetotal = {117}
}

@book{wellekTestingStatisticalHypotheses2010,
  title = {Testing {{Statistical Hypotheses}} of {{Equivalence}} and {{Noninferiority}}},
  author = {Wellek, Stefan},
  date = {2010-06-24},
  edition = {0},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/EBK1439808184},
  file = {C\:\\Users\\User\\Zotero\\storage\\QL94TD8X\\Wellek - 2010 - Testing Statistical Hypotheses of Equivalence and .pdf},
  isbn = {978-0-429-09267-1},
  langid = {english}
}

@article{whit11,
  title = {Multiple Imputation Using Chained Equations: {{Issues}} and Guidance for Practice},
  shorttitle = {Multiple Imputation Using Chained Equations},
  author = {White, Ian R. and Royston, Patrick and Wood, Angela M.},
  date = {2011-02-20},
  journaltitle = {Statistics in Medicine},
  shortjournal = {Stat Med},
  volume = {30},
  pages = {377--399},
  issn = {1097-0258},
  doi = {10.1002/sim.4067},
  abstract = {Multiple imputation by chained equations is a flexible and practical approach to handling missing data. We describe the principles of the method and show how to impute categorical and quantitative variables, including skewed variables. We give guidance on how to specify the imputation model and how many imputations are needed. We describe the practical analysis of multiply imputed data, including model building and model checking. We stress the limitations of the method and discuss the possible pitfalls. We illustrate the ideas using a data set in mental health, giving Stata code fragments.},
  eprint = {21225900},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\L74SBSX2\\White e.a. - 2011 - Multiple imputation using chained equations Issue.pdf},
  keywords = {Adolescent,Adult,Aged,Cardiovascular Diseases,Cholesterol,Female,Humans,Lipoproteins; HDL,Mental Health,Middle Aged,Models; Statistical,Multicenter Studies as Topic,Young Adult},
  langid = {english},
  number = {4}
}

@book{WhiteNoiseForecasting,
  title = {2.9 {{White}} Noise | {{Forecasting}}: {{Principles}} and {{Practice}}},
  shorttitle = {2.9 {{White}} Noise | {{Forecasting}}},
  url = {https://Otexts.com/fpp2/},
  urldate = {2020-06-04},
  abstract = {2nd edition},
  file = {C\:\\Users\\User\\Zotero\\storage\\9T77PMBZ\\wn.html}
}

@incollection{williamsonPhilosophyScienceIts2010,
  title = {The {{Philosophy}} of {{Science}} and Its Relation to {{Machine Learning}}},
  booktitle = {Scientific {{Data Mining}} and {{Knowledge Discovery}}: {{Principles}} and {{Foundations}}},
  author = {Williamson, Jon},
  editor = {Gaber, Mohamed Medhat},
  date = {2010},
  pages = {77--89},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-02788-8_4},
  abstract = {In this chapter I discuss connections between machine learning and the philosophy of science. First I consider the relationship between the two disciplines. There is a clear analogy between hypothesis choice in science and model selection in machine learning. While this analogy has been invoked to argue that the two disciplines are essentially doing the same thing and should merge, I maintain that the disciplines are distinct but related and that there is a dynamic interaction operating between the two: a series of mutually beneficial interactions that changes over time. I will introduce some particularly fruitful interactions, in particular the consequences of automated scientific discovery for the debate on inductivism versus falsificationism in the philosophy of science, and the importance of philosophical work on Bayesian epistemology and causality for contemporary machine learning. I will close by suggesting the locus of a possible future interaction: evidence integration.},
  file = {C\:\\Users\\User\\Zotero\\storage\\WQAQEAXZ\\Williamson - 2010 - The Philosophy of Science and its relation to Mach.pdf},
  isbn = {978-3-642-02788-8},
  keywords = {Certainty Factor,Inductive Logic Programming,Inductive Logic Programming System,Judgement Aggregation,Qualitative Evidence},
  langid = {english}
}

@book{Wood:2006,
  title = {Generalized Additive Models: An Introduction With},
  author = {Wood, Simon N.},
  date = {2006},
  publisher = {{Chapman \& Hall/CRC}},
  location = {{Boca Raton}}
}

@article{Yee:2009,
  title = {The  Package for Categorical Data Analysis},
  author = {Yee, Thomas W.},
  date = {2010},
  journaltitle = {Journal of Statistical Software},
  volume = {32},
  pages = {1--34},
  doi = {10.18637/jss.v032.i10},
  number = {10}
}

@article{youyouBirdsFeatherFlock2017,
  title = {Birds of a {{Feather Do Flock Together}}: {{Behavior}}-{{Based Personality}}-{{Assessment Method Reveals Personality Similarity Among Couples}} and {{Friends}}},
  shorttitle = {Birds of a {{Feather Do Flock Together}}},
  author = {Youyou, Wu and Stillwell, David and Schwartz, H. Andrew and Kosinski, Michal},
  date = {2017-03-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {28},
  pages = {276--284},
  issn = {0956-7976},
  doi = {10.1177/0956797616678187},
  abstract = {Friends and spouses tend to be similar in a broad range of characteristics, such as age, educational level, race, religion, attitudes, and general intelligence. Surprisingly, little evidence has been found for similarity in personality—one of the most fundamental psychological constructs. We argue that the lack of evidence for personality similarity stems from the tendency of individuals to make personality judgments relative to a salient comparison group, rather than in absolute terms (i.e., the reference-group effect), when responding to the self-report and peer-report questionnaires commonly used in personality research. We employed two behavior-based personality measures to circumvent the reference-group effect. The results based on large samples provide evidence for personality similarity between romantic partners (n = 1,101; rs = .20–.47) and between friends (n = 46,483; rs = .12–.31). We discuss the practical and methodological implications of the findings.},
  file = {C\:\\Users\\User\\Zotero\\storage\\8IB2ZFXN\\Youyou e.a. - 2017 - Birds of a Feather Do Flock Together Behavior-Bas.pdf},
  keywords = {close relationships,personality assessment,reference-group effect,similarity,social network},
  langid = {english},
  number = {3}
}

@article{youyouComputerbasedPersonalityJudgments2015,
  title = {Computer-Based Personality Judgments Are More Accurate than Those Made by Humans},
  author = {Youyou, Wu and Kosinski, Michal and Stillwell, David},
  date = {2015-01-27},
  journaltitle = {Proceedings of the National Academy of Sciences of the United States of America},
  shortjournal = {Proc Natl Acad Sci U S A},
  volume = {112},
  pages = {1036--1040},
  issn = {0027-8424},
  doi = {10.1073/pnas.1418680112},
  abstract = {This study compares the accuracy of personality judgment—a ubiquitous and important social-cognitive activity—between computer models and humans. Using several criteria, we show that computers’ judgments of people’s personalities based on their digital footprints are more accurate and valid than judgments made by their close others or acquaintances (friends, family, spouse, colleagues, etc.). Our findings highlight that people’s personalities can be predicted automatically and without involving human social-cognitive skills., Judging others’ personalities is an essential skill in successful social living, as personality is a key driver behind people’s interactions, behaviors, and emotions. Although accurate personality judgments stem from social-cognitive skills, developments in machine learning show that computer models can also make valid judgments. This study compares the accuracy of human and computer-based personality judgments, using a sample of 86,220 volunteers who completed a 100-item personality questionnaire. We show that (i) computer predictions based on a generic digital footprint (Facebook Likes) are more accurate (r = 0.56) than those made by the participants’ Facebook friends using a personality questionnaire (r = 0.49); (ii) computer models show higher interjudge agreement; and (iii) computer personality judgments have higher external validity when predicting life outcomes such as substance use, political attitudes, and physical health; for some outcomes, they even outperform the self-rated personality scores. Computers outpacing humans in personality judgment presents significant opportunities and challenges in the areas of psychological assessment, marketing, and privacy.},
  eprint = {25583507},
  eprinttype = {pmid},
  file = {C\:\\Users\\User\\Zotero\\storage\\RAY7FE23\\Youyou e.a. - 2015 - Computer-based personality judgments are more accu.pdf},
  number = {4},
  pmcid = {PMC4313801}
}

@article{yuShapeNoncentralChisquare2011,
  title = {The {{Shape}} of the {{Noncentral Chi}}-Square {{Density}}},
  author = {Yu, Yaming},
  date = {2011-06-26},
  url = {http://arxiv.org/abs/1106.5241},
  urldate = {2019-06-13},
  abstract = {A noncentral chi-square density is log-concave if the degree of freedom is nu{$>$}=2. We complement this known result by showing that, for each 0},
  archivePrefix = {arXiv},
  eprint = {1106.5241},
  eprinttype = {arxiv},
  file = {C\:\\Users\\User\\Zotero\\storage\\VRI965FJ\\Yu - 2011 - The Shape of the Noncentral Chi-square Density.pdf;C\:\\Users\\User\\Zotero\\storage\\FYLPLIK5\\1106.html},
  keywords = {60E05; 62E15; 33C10,Mathematics - Classical Analysis and ODEs,Mathematics - Statistics Theory},
  primaryClass = {math, stat}
}

@article{yuShapeNoncentralChisquare2011a,
  title = {The {{Shape}} of the {{Noncentral Chi}}-Square {{Density}}},
  author = {Yu, Yaming},
  date = {2011-06-26},
  url = {http://arxiv.org/abs/1106.5241},
  urldate = {2019-06-17},
  abstract = {A noncentral chi-square density is log-concave if the degree of freedom is nu{$>$}=2. We complement this known result by showing that, for each 0},
  archivePrefix = {arXiv},
  eprint = {1106.5241},
  eprinttype = {arxiv},
  keywords = {60E05; 62E15; 33C10,Mathematics - Classical Analysis and ODEs,Mathematics - Statistics Theory},
  primaryClass = {math, stat}
}

@article{Zeileis+Kleiber+Jackman:2008,
  title = {Regression Models for Count Data In},
  author = {Zeileis, Achim and Kleiber, Christian and Jackman, Simon},
  date = {2008},
  journaltitle = {Journal of Statistical Software},
  volume = {27},
  pages = {1--25},
  doi = {10.18637/jss.v027.i08},
  number = {8}
}

@article{zhangSystematicSurveyReporting2017,
  title = {A Systematic Survey on Reporting and Methods for Handling Missing Participant Data for Continuous Outcomes in Randomized Controlled Trials},
  author = {Zhang, Yuqing and Flórez, Ivan D. and Colunga Lozano, Luis E. and Aloweni, Fazila Abu Bakar and Kennedy, Sean Alexander and Li, Aihua and Craigie, Samantha and Zhang, Shiyuan and Agarwal, Arnav and Lopes, Luciane C. and Devji, Tahira and Wiercioch, Wojtek and Riva, John J. and Wang, Mengxiao and Jin, Xuejing and Fei, Yutong and Alexander, Paul and Morgano, Gian Paolo and Zhang, Yuan and Carrasco-Labra, Alonso and Kahale, Lara A. and Akl, Elie A. and Schünemann, Holger J. and Thabane, Lehana and Guyatt, Gordon H.},
  date = {2017-08-01},
  journaltitle = {Journal of Clinical Epidemiology},
  shortjournal = {Journal of Clinical Epidemiology},
  volume = {88},
  pages = {57--66},
  issn = {0895-4356},
  doi = {10.1016/j.jclinepi.2017.05.017},
  abstract = {Objective To assess analytic approaches randomized controlled trial (RCT) authors use to address missing participant data (MPD) for patient-important continuous outcomes. Study Design and Setting We conducted a systematic survey of RCTs published in 2014 in the core clinical journals that reported at least one patient-important outcome analyzed as a continuous variable. Results Among 200 studies, 187 (93.5\%) trials explicitly reported whether MPD occurred. In the 163 (81.5\%) trials that reported the occurrence of MPD, the median and interquartile ranges of the percentage of participants with MPD were 11.4\% (2.5\%–22.6\%).Among the 147 trials in which authors made clear their analytical approach to MPD, the approaches chosen included available data only (109, 67\%); mixed-effect models (10, 6.1\%); multiple imputation (9, 4.5\%); and last observation carried forward (9, 4.5). Of the 163 studies reporting MPD, 16 (9.8\%) conducted sensitivity analyses examining the impact of the MPD and (18, 11.1\%) discussed the risk of bias associated with MPD. Conclusion RCTs reporting continuous outcomes typically have over 10\% of participant data missing. Most RCTs failed to use optimal analytic methods, and very few conducted sensitivity analyses addressing the possible impact of MPD or commented on how MPD might influence risk of bias.},
  keywords = {Analytic approaches,Continuous outcome,Lost to follow-up,Missing participant data,MPD,Randomized controlled trials},
  langid = {english}
}

@article{zhu15,
  title = {Convergence {{Properties}} of a {{Sequential Regression Multiple Imputation Algorithm}}},
  author = {Zhu, Jian and Raghunathan, Trivellore E.},
  date = {2015-07-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {110},
  pages = {1112--1124},
  doi = {10.1080/01621459.2014.948117},
  abstract = {A sequential regression or chained equations imputation approach uses a Gibbs sampling-type iterative algorithm that imputes the missing values using a sequence of conditional regression models. It is a flexible approach for handling different types of variables and complex data structures. Many simulation studies have shown that the multiple imputation inferences based on this procedure have desirable repeated sampling properties. However, a theoretical weakness of this approach is that the specification of a set of conditional regression models may not be compatible with a joint distribution of the variables being imputed. Hence, the convergence properties of the iterative algorithm are not well understood. This article develops conditions for convergence and assesses the properties of inferences from both compatible and incompatible sequence of regression models. The results are established for the missing data pattern where each subject may be missing a value on at most one variable. The sequence of regression models are assumed to be empirically good fit for the data chosen by the imputer based on appropriate model diagnostics. The results are used to develop criteria for the choice of regression models. Supplementary materials for this article are available online.},
  file = {C\:\\Users\\User\\Zotero\\storage\\FPPUB4TU\\Zhu en Raghunathan - 2015 - Convergence Properties of a Sequential Regression .pdf;C\:\\Users\\User\\Zotero\\storage\\CJWQJ3FF\\01621459.2014.html},
  keywords = {Bayesian analysis,Chained equations,Compatible conditionals,Conditional specifications,Exponential family,Gibbs sampling,Missing data.},
  number = {511}
}

@software{zotero-290,
  type = {software}
}

@software{zotero-440,
  type = {software}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

